<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yitao&#39;s Blog</title>
  
  
  <link href="https://zjuytw.github.io/atom.xml" rel="self"/>
  
  <link href="https://zjuytw.github.io/"/>
  <updated>2021-08-09T10:06:13.523Z</updated>
  <id>https://zjuytw.github.io/</id>
  
  <author>
    <name>Wang Yitao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>6.824 Bitcoin lecture note</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-19.%20Bitcoin/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-19.%20Bitcoin/</id>
    <published>2021-08-09T09:13:23.361Z</published>
    <updated>2021-08-09T10:06:13.523Z</updated>
    
    <content type="html"><![CDATA[<h1>Bitcoin</h1><p>As mentioned in Satoshi Nakamoto’s paper that bitcoin is aimed to prevent double-spending as well as reduce the cost of third party involvement. </p><p>Bitcoin has three features that makes it Epoch-making</p><ul><li>Decentralization</li><li>Using Peer-to-Peer Technology</li><li>Low-cost transaction</li></ul><h2>Brief Intro </h2><p>Bitcoin is a distributed ledger which implement decentralization. Bitcoin ‘s design should solve Byzantine Generals Problem because it is running through public Internet. Bitcoin systems reply on <em>Proof of Work</em> to verify each node’s validation to prove itself running on a true CPU. Bitcoin also promise that : Malicious nodes’ blockchain won’t grow long, if most of nodes in the network are meritorious.</p><p>For double-spending problem, block-chain ensures that even if blockchain may fork at some point, but only one fork will be accepted in the end.</p><h3>Drawbacks</h3><ul><li>Every new transaction need 10 min before recording on the blockchain</li><li>The trustworthiness grows as the chain grows longer, but still have chance to be waived by receiving a longer chain from other node.</li><li>Waste energy</li></ul><h2>Extends</h2><p>You may read paper and watch lecture to get more detailed information about bitcoin.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Bitcoin&lt;/h1&gt;

&lt;p&gt;As mentioned in Satoshi Nakamoto’s paper that bitcoin is aimed to prevent double-spending as well as reduce the cost of</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
    <category term="Bitcoin" scheme="https://zjuytw.github.io/tags/Bitcoin/"/>
    
  </entry>
  
  <entry>
    <title>6.824 CT lecture note</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-18.%20Certificate%20Transparency(Fork%20Consistency)/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-18.%20Certificate%20Transparency(Fork%20Consistency)/</id>
    <published>2021-08-09T09:13:23.356Z</published>
    <updated>2021-08-09T10:45:57.253Z</updated>
    
    <content type="html"><![CDATA[<h1> Certificate Transparency</h1><h2>Introduce</h2><p>First review the vulnerability of HTTP and an example of  Man in the Middle Attack.</p><h4> HTTP</h4><blockquote><p>HTTP is a request response protocol to communicate asynchronously between client and server.</p><p>For websites and pages the browser acts as a client and a web-server like Apache or IIS acts as server. The server hosts the files (like html , audio , video files etc) and returns are responds to client requests with the data. Depending on the request a response contains the status of the request.</p><p>The process involves a series of messages that go back and forth between the client and server. The process starts with initiating a connection. After that a process known as TCP slow start kicks in. At this point data is passed between the two parties via data packets and often requires multiple round trips.</p><p>TCP slow start is designed to gradually expand the amount of data traversing the wire each round trip. The initial packet size is 16kb and doubles on subsequent round trips until a max size is reached. This can vary, but tends to be around 4MB for most connections.</p><p>This process is used because the server does not know how much bandwidth the client can handle. Rather than overflowing the client the server uses a gentle size and continues to increase until a limit is found.</p><p>As data or request bodies move between the client and the server it is done using clear or plain text. This is viewable by anyone or software watching the network traffic.</p><p>This is not that important for general content. However, today even when you don’t think sensitive data is moving between the two parties more sessions do transport identifying information. This is why every website should use TLS to secure HTTP connections.</p></blockquote><h4> Man in the Middle Attack</h4><p><img src="/2021/08/09/6.824-18.%20Certificate%20Transparency(Fork%20Consistency)/man-in-the-middle-mitm-attack.png" alt="man-in-the-middle-mitm-attack"></p><p>A third-party may easily hijack the connection towards target website and redirect to its own rogue web, for no further check mechanism in HTTP.</p><h4>Certificate, SSL, TLS, HTTPS</h4><p><strong>HTTPS work flow:</strong></p><p><img src="https-connection-sequence-diagram.png" alt="https-connection-sequence-diagram"></p><p>Particularly, server should request a certificate from <em>CA(Certificate Authority)</em>. Whenever client send a connection request to server, it will receive a CERT from server.</p><table><thead><tr><th>Certificate</th></tr></thead><tbody><tr><td>Server name, eg: “gmail.com”</td></tr><tr><td>Public Key of server</td></tr><tr><td>CA’s Signature</td></tr></tbody></table><p>CA’s unique signature ensures that just CA can issue the certificate to server that no one else can counterfeit.</p><p><strong>NOTE:</strong> A vulnerability of this scheme is that once a CA was broken into or something else happened and caused CA issued a malicious certificate. Client may have chance to talk to a rogue web and info may get leaked.</p><h2>Certificate Transparency</h2><p>Certificate Transparency is a system that stores certificate logs which are stored distributed and append-only on CT. CT can provide user certificate verification to keep CA from issuing malicious certificate and the certificate even keep in CA for longer time.</p><p><img src="/2021/08/09/6.824-18.%20Certificate%20Transparency(Fork%20Consistency)/CT_work_flow.png" alt="CT work flow"></p><p>CT promises following:</p><ul><li>Certificates are deposited in public, transparent logs</li><li>Logs are cryptographically monitored</li></ul><h3>Implementation</h3><p>Each certificates are stored as a node in Merkle Tree in CT. <img src="/2021/08/09/6.824-18.%20Certificate%20Transparency(Fork%20Consistency)/MerkleTree.png" alt="Merkle Tree"></p><p>Each node in each level is the value of the output of <em>cryptographic hash function</em> that maps an arbitrary-size message <em>M</em> to a small fixed-size output H(<em>M</em>), with the property that it is infeasible in practice to produce any pair of distinct messages <em>M1</em> ≠ <em>M2</em> with identical hashes H(<em>M1</em>) = H(<em>M2</em>). And the we have</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h(0, K) = H(record K)</span><br><span class="line">h(L+1, K) = H(h(L, 2 K), h(L, 2 K+1))</span><br></pre></td></tr></table></figure><p>With the property of above, we can determine whether a specific certificate stored in the tree, we can recompute hash(<em>the certificate</em>) and the hash value of its siblings and relatives to finally get the top-level’s hash. If H(4,0) == recomputed H(4,0), then proved.</p><p><strong>Example:</strong></p><p>For example, suppose we want to prove that a certain bit string <em>B</em> is in fact record 9 in a tree of 16 records with top-level hash <em>T</em>. We can provide those bits along with the other hash inputs needed to reconstruct the overall tree hash using those bits. Specifically, the client can derive as well as we can that:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">T = h(4, 0)</span><br><span class="line">= H(h(3, 0), h(3, 1))</span><br><span class="line">= H(h(3, 0), H(h(2, 2), h(2, 3)))</span><br><span class="line">= H(h(3, 0), H(H(h(1, 4), h(1, 5)), h(2, 3)))</span><br><span class="line">= H(h(3, 0), H(H(H(h(0, 8), h(0, 9)), h(1, 5)), h(2, 3)))</span><br><span class="line">= H(h(3, 0), H(H(H(h(0, 8), H(record 9)), h(1, 5)), h(2, 3)))</span><br><span class="line">= H(h(3, 0), H(H(H(h(0, 8), H(B)), h(1, 5)), h(2, 3)))</span><br></pre></td></tr></table></figure><h3> Fork Attack (Fork Consistency)</h3><p>The proof of fork consistency, image a log servers have a chain of logs and once the log server wants to fork (like, to trick a user for a malicious certificate but not seen by other monitors etc.) CT has a mechanism to detect the inconsistency by <em>gossip</em></p><p><img src="/2021/08/09/6.824-18.%20Certificate%20Transparency(Fork%20Consistency)/ForkConsistencyEg.png" alt="Fork Consistency eg"></p><p>Like the example, once a log server has a fork line that starts with B for bogus and have a current STH(signed tree head, the top-level hash value). We can simply calculate if STH1’s log a prefix log of STH2’s log by the same way prove if STH1’s log is inside STH2’s log tree.</p><p>If return a false, which means STH1 is on a different fork.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt; Certificate Transparency&lt;/h1&gt;

&lt;h2&gt;Introduce&lt;/h2&gt;

&lt;p&gt;First review the vulnerability of HTTP and an example of  Man in the Middle Attac</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
    <category term="Certificate Transparency" scheme="https://zjuytw.github.io/tags/Certificate-Transparency/"/>
    
  </entry>
  
  <entry>
    <title>6.824 COPS lecture note</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-17.COPS(Causal%20Consistency)/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-17.COPS(Causal%20Consistency)/</id>
    <published>2021-08-09T09:13:23.348Z</published>
    <updated>2021-08-09T10:06:00.232Z</updated>
    
    <content type="html"><![CDATA[<h1> COPS</h1><h2>Introduce</h2><p>Still the website Front-End read from DB storage cluster model. We are gonna explore another possibility as local read and local write to speed up transactions. How to achieve <em>stronger</em> Consistency in terms of CAP Theorem.</p><p>Consider [Spanner](./13.Spanner(Strong Consistency RW).md) and [Memcache](./16.Memcache(Cache Consistency).md)’s scheme</p><ul><li>Spanner</li></ul><blockquote><p>Linearizability</p><p>For R/W transaction, Use PAXOS and 2PC to write to remote replication. (Wait for quorum’s acknowledge)</p><p>For R/O transaction, read from local. Use Snapshot Isolation and TrueTime clock mechanism to ensure local serializability.</p></blockquote><ul><li>Memcache</li></ul><blockquote><p>Eventual Consistency</p><p>Introduce memcache layer to achieve write to DB and read from memcache.</p><p>Write need receive DB’s acknowledge and read has no need to wait.</p></blockquote><p>For COPS,  it figures out a new way to make more efficient read and write by implementing <em>causal consistency</em>, which is stronger than eventual consistency but weaker than linearizability.</p><h2>Implementation</h2><p><img src="/2021/08/09/6.824-17.COPS(Causal%20Consistency)/COPS.png" alt="COPS"></p><p>COPS, aka Cluster of Order-Preserving Servers, a key-value store that delivers causal-consistency model across the wide-area. Every Data Center has a local COPS cluster, which maintains a complete replica of stored data. So client can just talk to local cluster for data’s read and write.</p><p>In each Data Center, data are divided into many shards, which is linearizable and clients access each partition independently. So the whole cluster is linearizable as well. The problem comes when clusters communicate with each other to remain sync. </p><p>To achieve causal consistency, COPS have a prime node be responsible for local writing. After local writing is finished, prime will send it to other cluster’s prime node, and a version number will be sent as well to keep causal order.</p><h2> Causality</h2><p>​     <em>Potential Causality definition</em> </p><ul><li><p><strong>Execution Thread</strong>. If a and b are two operations in a single thread of execution, then a -&gt; b if operation a happens before operation b</p></li><li><p><strong>Gets From</strong>. If a is a put operation and b is a get operation that returns the value written by a, then a -&gt; b</p></li><li><p><strong>Transitivity</strong>. For operations a, b, and c, if a -&gt; b and b -&gt; c, then a -&gt; c</p></li></ul><p><strong>Example</strong>:</p><p><img src="/2021/08/09/6.824-17.COPS(Causal%20Consistency)/causal.png" alt="causal"></p><p>We can learn from the figure that <em>put(z,5)</em> is derived from <em>get(x) = 4</em>, which means to execute <em>put(z,5)</em> we have ensured that what is logically happened earlier than it. </p><p><strong>Note:</strong> If the system can not tell weather two operation’s happening order, since there is no explicit reason about they, We can simply define they are concurrent, so system can decide the order they happen. But for two <em>put</em> concurrently write to the same key, there is a conflict.</p><p>So to deal with conflicts, <code>Last write win</code> is a proper way to deal with it but if we want to do<code>append &amp; atomical count</code> like thing, this strategy may not work.</p><h2> Context</h2><p>Each client maintains a context to explicitly mark their state. After each operation, the client will append a entry of keys’ version. So DC can  simply use this entries to verify the dependencies of one operation.</p><p><strong>Example</strong>:</p><p>We have 3 Data Centers, and one client calls put(Z, value, Zver3, Xver2, Yver4) to put Z = value. To forward this operation to other DC, DC1 will check the dependencies of Zver3, Xver2, Yver4 in other DC, if others’ are not in this stage, it will wait till other DCs reach or exceed the dependencies’ version number</p><h2>Lamport Timestamp</h2><p>To achieve global order, COPS use <code>Lamport Timestamp in higher bits + unique ID For Data Center in lower bits</code> . Combining with Logical timeclock and Wall Clock, we can give a global sequence despite of large inaccuracy.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tmax = highest version seen (from self and others)</span><br><span class="line">T = max(Tmax + 1, wall-clock time)</span><br></pre></td></tr></table></figure><h2>Write</h2><p>Client -&gt; Local Data Store Cluster -&gt; other DCs</p><p>When client sends a <em>put(key, value …)</em> , client library will calculate the dependencies according to the context. The local prime will wait till cluster has indeed store all the dependencies( check by version number). Then send to remote clusters, do the same.</p><h2>Read</h2><p>Read from local cluster, the library function provide both read the latest version of key and a specific older one by explicitly send a context with get.</p><h2> limitation </h2><ul><li><p>Causal Consistency can not be aware of external dependency. For example, Alice told Bob to check a new status of the key, and then Bob sent a get request via client. Now Bob may see old value of key because the system do not know that Alice calls Bob yields Bob’s get request.</p><p>And this is also discussed by lamport in &lt;Time, Clocks, and the Ordering of Events in a Distributed System&gt;</p></li><li><p>It’s hard to manage conflict, <code>Last write win</code> is not generic </p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt; COPS&lt;/h1&gt;

&lt;h2&gt;Introduce&lt;/h2&gt;

&lt;p&gt;Still the website Front-End read from DB storage cluster model. We are gonna explore another possibil</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
    <category term="COPS" scheme="https://zjuytw.github.io/tags/COPS/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Memcache lecture note</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-16.Memcache(Cache%20Consistency)/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-16.Memcache(Cache%20Consistency)/</id>
    <published>2021-08-09T09:13:23.342Z</published>
    <updated>2021-08-09T10:05:56.458Z</updated>
    
    <content type="html"><![CDATA[<h1>Memcahce at Facebook </h1><h2>Intro</h2><p>In a popular webserver scenario, we have web application that clients send request (especially most reads and a few writes) to Data Base, and as we both know things get worse when one peer in the system suffer a throughput bottleneck. To achieve better performance and also get stronger consistency.</p><ul><li>Single Web Server(eg, running on Apache Tomcat) + Single  DataBase(eg, MySQL/ Oracle)</li></ul><p>​        &#8595; </p><ul><li><p>Muti-Stateless Web Server + Single DB</p><p>&#8595;</p></li><li><p>Mutl-Stateless Web Server + DB cluster(sharded  by key, in both scheme and table layer)</p></li></ul><p>​       &#8595;</p><ul><li>Mutl-Stateless Web Server + Memcache (For speeding up reads) + DB cluster</li></ul><h2>Implementation</h2><p>Facebook noticed that their customers consume an ordered of magnitude more content that they create, so fetching data is the domain element for the performance bottleneck. Also, they have various storage services, like MySQL, HDFS etc, which means a flexible caching strategy is needed.</p><p>Finally, they came up with an architecture that separate caching layer from the persistence layer, which means that for a group a Web server, they combine with a group of Memcache to form a <em>Front-End Cluster</em>, then a Front-End Cluster combine with a data-completed DB to form a <em>region</em>(AKA Data Center). So as the distributed spread of region, users from different area of USA can access to the Web server with lowest latency by choosing different region.</p><p><img src="/2021/08/09/6.824-16.Memcache(Cache%20Consistency)/Memcache1.png" alt="Memcache1"></p><p>Because of the tolerance of stale message differs in different situation</p><ul><li>User can stand for transient stale data, but not too long</li><li>User tend to observe their latest data after writing it</li></ul><p>So the Memcache can achieve eventual consistency by using its R/W strategy.<img src="/2021/08/09/6.824-16.Memcache(Cache%20Consistency)/Memcache2.png" alt="Memcache2"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Read Scheme:</span><br><span class="line">v = get(k)</span><br><span class="line">if v == nil</span><br><span class="line">v = fetch from DB</span><br><span class="line">set(k,v)</span><br><span class="line"></span><br><span class="line">Write Scheme:</span><br><span class="line">send k,v to DB</span><br><span class="line">delete(k) in MC</span><br></pre></td></tr></table></figure><h4>Hint</h4><ul><li><p>This scheme can not prevent users from seeing stale data</p><ul><li>If user read exactly after <em>line 8</em>, at this point, Memcache still holds the stale data but DB has updated the key to the new value</li></ul></li><li><p>Q: Why not delete key in the MC first before <em>send k,v to DB</em>?</p><ul><li>A: Because if at the time deleted the key in MC but another server did not see key in MC, it will send fetch to DB then may get the stale data that might be deleted afterwards and store to MC. Then MC may store the stale data until another write is fired up.</li></ul></li><li><p>Q: Why not just set(k,v) in MV in <em>line 9</em></p><ul><li>A : Because delete is idempotent while set is not. Check in the example:</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">C1 : x = 1 -&gt; DB</span><br><span class="line">C2 : x = 2 -&gt; DB</span><br><span class="line"> set(x,2)</span><br><span class="line">C1 : set(x,1)</span><br><span class="line">// makes stale data stored</span><br></pre></td></tr></table></figure></li><li><p>Prime &amp; Secondary Scheme</p><ul><li>For many regions, there is one master region and many salve region</li><li>Local Read and Prime Write<ul><li>For read, each FE read use <em>Read Scheme</em> in local region. This is super fast</li><li>For write, slave’s write need to be send to primary region</li></ul></li><li>Prime&amp;Secondary replication, primary DB always send info to remote DB to stay in sync</li></ul></li></ul><h2>Performance</h2><p>Let’s talk about two parallel process strategies.</p><ul><li>Partition<ul><li>increase RAM efficiency that each Key just store once </li><li>Not good for  some hot keys</li><li>Client may talk to many part for one website’s resource</li></ul></li><li>Replication<ul><li>Good for hot key</li><li>Fewer TCP connection</li><li>RAM wasted for more replica</li></ul></li></ul><p>For Facebook’s architecture, we have two completed replicated asynchronized region that brings fault-tolerance also low-latency for different area’s user. In each region, FB partitioned DB then using many Memcache to cache hot keys to reduce DB’s load. There is also a regional Memcache cluster in each region to cache those not too hot keys.</p><h2> Lease</h2><p> FB uses lease mechanism to fix the <em>Thunder Herd and Race Condition</em>.</p><ul><li><p>Thunder Herd – If many FE are simultaneously read the same key from Memcache and at this time, one FE do a write() and delete the old key in Memcache. Then DB may have the risk of flooded by too many queries for one key.</p></li><li><p>Race Condition – Example</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">C1 : get(k) -&gt; miss</span><br><span class="line">C1 : read k from DB -&gt; value1</span><br><span class="line">C2 : write k = value2 -&gt; DB</span><br><span class="line">C2 : delete(k) to MC</span><br><span class="line">C1 : set(k,v1) to MC</span><br><span class="line">// In this situation, stale data of value1 will store on MC forever</span><br></pre></td></tr></table></figure><h4>Solution</h4><p>To each get(k), Memcache server should issue FE a lease for a period of time.</p><ul><li>Thunder Herd, if one FE get the lease, then others that also send get(k) will block till the first FE calls put(k,v, l) or lease expired</li><li>Race Condition, C1’s get(k) will be issued a lease, but C2’s delete will invalid the old lease, the when C1 fetch value1 from DB then calls put(k,v1, l), the Memcache server will reject it.</li></ul></li></ul><h2>Extend</h2><p>Another introduce of twitter’s cache system in <a href="https://tanxinyu.work/twitter-cache-analysis-thesis/">Twitter 内存缓存系统分析论文阅读</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Memcahce at Facebook &lt;/h1&gt;

&lt;h2&gt;Intro&lt;/h2&gt;

&lt;p&gt;In a popular webserver scenario, we have web application that clients send request (espec</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
    <category term="Memcache@FB" scheme="https://zjuytw.github.io/tags/Memcache-FB/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Spark lecture note</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-15.Spark(Big%20Data%20Process)/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-15.Spark(Big%20Data%20Process)/</id>
    <published>2021-08-09T09:13:23.335Z</published>
    <updated>2021-08-09T10:06:04.918Z</updated>
    
    <content type="html"><![CDATA[<h1>Spark</h1><h2>Introduce</h2><p>Spark is a successor of MapReduce to process distributed big-data computing tasks.  Frameworks like MapReduce, Dryad, and Spark help data scientists to focus on their business rather than wasting time on designing the distributed tasks and fault-tolerance. </p><p>There are some constraints in previous frameworks that MapReduce lacks abstractions for leveraging distributed memory so makes it inefficient for those that reuse intermediate results across multiple computations and lacks high interactive flexibility, programmers may have trouble implementing some complex algorithms. </p><p>Spark is an implementation of a new abstraction called resilient distributed datasets(RDDs), which are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators.</p><h2> RDD</h2><p>RDD(Resilient Distributed Dataset) is a collection of Read-Only and Partitioned records. RDDs can only be created through deterministic operations on either 1) data in stable storage or 2) other RDDs. Spark uses <em>Lineage</em> to keep track of how each RDD is transformed from previous datasets.<img src="/2021/08/09/6.824-15.Spark(Big%20Data%20Process)/lineage.jpg" alt="lineage"></p><p>Spark provides <em>Action</em> as well as <em>Transformation</em>. Action calculate RDDs and gets a result. Transformation imports data from external sources or transform an old RDD to a new Read-Only RDD.</p><p><img src="/2021/08/09/6.824-15.Spark(Big%20Data%20Process)/Transformation_Action.jpg" alt="Transformation_Action"></p><h2>Computation Schedule</h2><p>RDDs are stored in distributed servers, so when we need to do Transformation,  systems need to fetch previous RDD in the corresponding servers. There are two kinds of Transformations that forms different dependency between RDDs</p><ul><li>Narrow Dependency : Each partition of the parent RDD is used by at most one partition of  the child RDD.</li><li>Wide Dependency : Multiple child partitions may depend on the parent RDD.</li></ul><p><img src="/2021/08/09/6.824-15.Spark(Big%20Data%20Process)/Dependencies.png" alt="Dependencies"></p><p>Spark speeds up Transformation by optimizing the Transformations related to <em>Narrow Dependency</em>. First, narrow dependencies allow for pipelined execution on one cluster node, which can compute all the parent partitions.  Second, recovery after a node failure is more efficient with a narrow dependency, as only the lost parent partitions need to be recomputed.</p><p>In contrast, in a lineage graph with wide dependencies, a single failed node might cause the loss of some partition form all the ancestors of an RDD, requiring a complete re-execution.</p><p>Overall, a RDD are consistent of  the following elements:</p><ul><li>Its partitions</li><li>Its parent partitions</li><li>Transformation</li><li>Its metadata(eg, data type, storage position etc.)</li></ul><p><img src="/2021/08/09/6.824-15.Spark(Big%20Data%20Process)/stage.jpg" alt="stage"></p><p>When user calls Action to process computation on RDD, Spark will build different stages according to lineages. Hence, Spark can build a job stage that contains as many Narrow Dependencies as possible to speed up the whole system’s efficiency. The boundaries of the stages are the shuffle operations required for wide dependencies, or any already computed partitions that can short-circuit the computations of a parent RDD. After building the job stages, Spark then launches tasks to compute missing partitions from each stage until it has computed the target RDD.</p><p>While scheduling Tasks, Spark assigns tasks to machines based on data locality. The task will directly be processed by those nodes that is already holds the partition needed in memory. Otherwise, if a task processes a partition for which the containing RDD provides preferred locations(eg, an HDFS file), we send it to those.</p><h2>Fault-Tolerance</h2><p>Spark can re-compute the content of a failed RDD by dependencies from lineage graph. But there is a wide dependency during the re-computation, which means we have to re-compute all the RDD it depends, also, Spark won’t store all the RDD in the memory, or it will soon run out of memory. So we have to manually do persist, if necessary. </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.persist(REPLICATE)</span><br></pre></td></tr></table></figure><h2>Conclusion</h2><p>Spark RDD has the feature of:</p><ul><li>Store all info directly on memory</li><li>Interactive API</li><li>Find both Narrow and Wide Dependencies, while narrow one is more efficiency</li><li>Have Checkpoint to failover from wide dependency failure</li></ul><p>But we still need to be aware that Spark is not a replacement for MapReduce: For those model and algorithms already fit in MapReduce, Spark won’t have a more efficient result for them.</p><p>&lt;h2</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Spark&lt;/h1&gt;

&lt;h2&gt;Introduce&lt;/h2&gt;

&lt;p&gt;Spark is a successor of MapReduce to process distributed big-data computing tasks.  Frameworks like M</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
    <category term="Spark" scheme="https://zjuytw.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>6.824 CRAQ lecture note(Pending for updated)</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-14.OCC(FaRM)/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-14.OCC(FaRM)/</id>
    <published>2021-08-09T09:13:23.329Z</published>
    <updated>2021-08-09T10:44:28.217Z</updated>
    
    <content type="html"><![CDATA[<h1>FaRM (To be updated...)</h1><h2>Introduce</h2><p>Microsoft’s main memory distributed computing platform, FaRM can provide distributed transactions with serializability, high performance, durability, and high availability using RDMA and a new, inexpensive approach to providing non-volatile DRAM.</p><h3>NVRAM</h3><ul><li>Strategy to become non-volatile:<ul><li>Using battery as back-up power, once the electric power fails, the system goes with battery and do all context saving work then shut down</li><li><strong>Note:</strong> This scheme is just helpful when encounters power failure, is not applicable for hardware/ software failure. –Because otherwise the system just shut down directly.</li></ul></li></ul><h2>Optimistic Concurrency Control</h2><p>For a optimistic lock, we have</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Xaction begin</span><br><span class="line">read all values without lock</span><br><span class="line">use a buffer to store write</span><br><span class="line">commit write</span><br><span class="line">Xaction end</span><br></pre></td></tr></table></figure><p>Before commit transaction to storage, system need to verify the validation of the transaction. If success, then commit, else abort all the operation related to transaction.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tx.create()</span><br><span class="line">o = tx.copyRead(OID)</span><br><span class="line">o.value += 1</span><br><span class="line">tx.write(OID, o)</span><br><span class="line">ok := tx.commt()</span><br></pre></td></tr></table></figure><h2>Transaction Management</h2><p><img src="/2021/08/09/6.824-14.OCC(FaRM)/Farm1.jpg" alt="Farm1"></p><p> <img src="/2021/08/09/6.824-14.OCC(FaRM)/OCC_commit_protocol.png" alt="OCC commit protocol"></p><p>Refer to the above figures about server layout and OCC commit protocol. Let’s talk about FaRM’s transaction management. FaRM uses OCC and 2PC to achieve its serializability.</p><p><strong>2PC strategy</strong></p><ul><li> Read without lock, read(&amp; value, &amp;version)</li><li>use one-side RDMA to read</li><li>Lock the revised data<ul><li>primary polling for data which is (use DRMA to poll)<ul><li>locked, so send reject</li><li>VERS changed, then send reject</li><li>else, then set the lock and send yes.( To avoid racing, use CHECK&amp;SET atomic operation here)</li></ul></li></ul></li><li>do validation for those no changed shard<ul><li>To check if version changed or locked]</li></ul></li><li>Commit</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;FaRM (To be updated...)&lt;/h1&gt;

&lt;h2&gt;Introduce&lt;/h2&gt;

&lt;p&gt;Microsoft’s main memory distributed computing platform, FaRM can provide distribute</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
    <category term="FaRM" scheme="https://zjuytw.github.io/tags/FaRM/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Spanner lecture note</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-13.Spanner(Strong%20Consistency%20RW)/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-13.Spanner(Strong%20Consistency%20RW)/</id>
    <published>2021-08-09T09:13:19.948Z</published>
    <updated>2021-08-09T10:05:42.935Z</updated>
    
    <content type="html"><![CDATA[<h1>Spanner </h1><h1>Introduce</h1><p>Spanner is Google’s scalable, multi-version, globally-distributed, and synchronously-replicated data. Spanner is the first global distributed Database and support external consistency. It is a big framework, but I just want to focus on some particular points – Mainly about external consistency and True-Time mechanism.</p><p>You may look for more detailed illustration on Google’s Spanner paper or other’s blog about spanner.</p><h2>Consistency</h2><p><img src="/2021/08/09/6.824-13.Spanner(Strong%20Consistency%20RW)/spanner1.webp" alt="spanner1"></p><p>Assume that if we have 3 Data Centers, we have to send data across all data centers to keep consistency.  To keep a strong consistency, Google uses paxos to send logs and reach consensus. Moreover, google has its True-Time mechanism to reach external consistency, but let’s talk about it later.</p><h3>R/W transactions</h3><p>For a transaction both need to read and write,spanner uses 2PC.</p><ul><li>Pick a unique transaction ID to identify</li><li>Do all the reads first then do all writes</li><li>Send read request to all leaders in DCs, then DC lock the resources and reply</li><li>Choose a leader as transaction coordinator</li><li>Send writes request to all leaders, and leader gonna send prepared msg to followers into paxos log to make sure leader isn’t crashed and lost lock</li><li>Once one leader finishes promising to followers, it sends a Yes to the coordinator</li><li>Once coordinate received all Yes signal, it start to commit writes to its followers</li><li>Then tell other leader to commit</li></ul><h3>R/O transactions</h3><p>For Read-Only transactions, spanner speeds up this type by no 2PC and no locks.</p><blockquote><p> Start with a question that why we not directly read the latest value of each key needed?</p><p>Answer: </p><p>For a transaction T3 that print(x,y), if we have the timeline of below:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">T1 :  wx,wy,commit</span><br><span class="line">T2 :                      wx,wy,commit               </span><br><span class="line">T3 :                 Rx                 Ry</span><br></pre></td></tr></table></figure><p>T3 just saw x,y yielded by different transaction which breaks the serializability.</p></blockquote><p>From the example above, we know that our read need to fetch data in the same version. So spanner need to at least reach level of <em>Snapshot Isolation</em>.</p><pre><code>&lt;h4&gt; Snapshot Isolation&lt;/h4&gt;</code></pre><p>Spanner gives each transaction a timestamp, which makes all transactions execute in Timestamp order.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">R/W&#x27;s TS = commit time</span><br><span class="line">R/O&#x27;s TS = start time</span><br></pre></td></tr></table></figure><p> Because Spanner has a multi-versions DB, that stores many versions (Not all version but a transient period’s versions). For R/O Xactions, DB can find the value with latest version less than R/O’s start time.</p><p><strong>Example:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">T1 @TS10:  wx,wy,commit()</span><br><span class="line">                    ⬆x@10 = 9, y@10 = 10</span><br><span class="line">T2 @TS20:                      wx,wy,commit</span><br><span class="line">                               ⬆x@20 = 8, y@10 = 12</span><br><span class="line">T3 @TS15:                 Rx                 Ry</span><br><span class="line">   ⬆ read the TS of 10</span><br></pre></td></tr></table></figure><blockquote><p>Q: what if local replica is minority, how to get latest version less than TS?</p><p>A: Every paxos peer gets log from leader, if one follower’s last log’s Timestamp &lt; TS, it will wait for leader’s msg till last log’s TS exceeds required TS</p></blockquote><h2> True-Time mechanism</h2><p>Because of working volts and inner CPU frequency , it is likely every DC’s time can not keep sync without outside’s help. We have two consequence:</p><blockquote><ol><li><p>R/O transaction’s TS too large</p><p> Answer: correct in practice but slow, it will wait for paxo replicas to catch up</p></li><li><p>R/O transaction’s TS too small</p><p>Answer: it may miss recent writes, and not external consistent</p></li></ol></blockquote><p>In Spanner’s scheme, Google has a satellites to keep synchronize its official time to each DC.  In considering of latency in transportation, Spanner give each TT a range to mark its {earliest, latest} arrival time.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TT interval = [Earliest, Latest]</span><br></pre></td></tr></table></figure><p>And we have a start rule: </p><ul><li><p>TS = TT.now(). latest</p></li><li><p>For R/O, TS is the latest TT on start</p></li><li><p>For R/W, TS is the latest TT on commit</p></li><li><p>Commit Delay strategy</p><ul><li>R/W transaction delays till transaction’s commit time &lt; TS.now().earliest</li><li>To make sure commit time in the past.</li></ul></li></ul><h2>Extends</h2><p>There are many details I haven’t covered, if you are interest in them. You can just search on Google for English Blogs, as for Chinese Blogs, I do recommend <a href="https://www.jianshu.com/p/6ae6e7989161">Google-Spanner 论文的思考</a> and <a href="https://toutiao.io/posts/zdqrx0/preview">Spanner, 真时和CAP理论</a></p><p>​    </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Spanner &lt;/h1&gt;

&lt;h1&gt;Introduce&lt;/h1&gt;

&lt;p&gt;Spanner is Google’s scalable, multi-version, globally-distributed, and synchronously-replicated da</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
    <category term="Spanner" scheme="https://zjuytw.github.io/tags/Spanner/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Frangipani lecture note</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-11.Frangipani(Cache%20Consitency)/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-11.Frangipani(Cache%20Consitency)/</id>
    <published>2021-08-09T09:12:42.351Z</published>
    <updated>2021-08-09T10:05:31.237Z</updated>
    
    <content type="html"><![CDATA[<h1>Frangipani(To be updaed)</h1><h2>Introduce</h2><p>While Frangipani is a paper published in 1997, I just want to talk about cache consistency in detail.</p><p>The paper are mainly discussed following 3 aspects:</p><ul><li>cache coherence</li><li>distributed transactions</li><li>distributed crash recovery</li></ul><h2>Cache consistency</h2><p>Rules for cache coherence</p><ul><li>No cached data without data’s lock</li><li>acquire lock then read from petal</li><li>write to petal then release lock</li></ul><h2>Extends</h2><p>For more details, you may refer to this Blog<a href="https://www.cnblogs.com/jamgun/p/14668522.html">Frangipani: A Scalable Distributed File System 论文阅读</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Frangipani(To be updaed)&lt;/h1&gt;

&lt;h2&gt;Introduce&lt;/h2&gt;

&lt;p&gt;While Frangipani is a paper published in 1997, I just want to talk about cache con</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
    <category term="Frangipani" scheme="https://zjuytw.github.io/tags/Frangipani/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Aurora lecture note</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-10.Aurora/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-10.Aurora/</id>
    <published>2021-08-09T09:11:36.427Z</published>
    <updated>2021-08-09T10:05:22.628Z</updated>
    
    <content type="html"><![CDATA[<h1>Aurora</h1><h2>Introduce</h2><p>Amazon Aurora is a relational database service for OLTP workloads offered as part of Amazon Web Services. Aurora is designed to address the constraint of bottleneck of network throughput, it also allows for fast crash recovery, failovers to replicas and fault-tolerant.</p><h2> History</h2><h3> EC2</h3><p>EC2 is Elastic Cloud 2 for short. Users can rent instances of EC to deploy their Web Server or DB services. </p><p>Each instance of EC2 are running in the Virtual Machine on the physical node, and all their storage is redirected to a external locally attached disk via VMM(Virtual Machine Monitor). </p><ul><li>For stateless web server, EC2 is convenient for its scalability and high-performance</li><li>For a storage system like DB service, there are bunch of contraint:<ul><li>Limited expansion : MySQL on EC2 is not able to do write expansion</li><li>Limited fault-tolerance : Once the node fails, we can not access to locally attached disk for data</li></ul></li></ul><p><img src="/2021/08/09/6.824-10.Aurora/EC2.png" alt="EC2"></p><h3>EBS</h3><p>EBS is Elastic Block Store for short. It is the progress of EC2 that Amazon uses a multiple instances of EBS to do a <em>Chain-Replication</em> to have fault-tolerance.</p><p>Constraints:</p><ul><li>Network bottleneck because of large amount of data is sending by network</li><li>Not FT, for Amazon always put EBS in same Data Center.</li></ul><h3>RDS</h3><p>To deal with the constraints mentioned above, Amazon provides a more fault-tolerance system,  Relational Database Service<img src="/2021/08/09/6.824-10.Aurora/Aurora-RDS.png" alt="Aurora-RDS"></p><p>Compared with EBS, RDS can survive if a whole AZ(Available Zone) fails, but have to send write between primary and replica, which means the performance of write decreases as well as the data of cross-AZ increases dramatically.</p><h2> Aurora</h2><p>For a new system, Amazon was eager to have both fault-tolerance and performance done well, as following:</p><ul><li>Write although one AZ down</li><li>Read although one AZ down + one replica down</li><li>Minor slow won’t affect overall efficiency</li><li>Fast Re-replication</li></ul><p><img src="/2021/08/09/6.824-10.Aurora/Aurora.png" alt="Aurora"></p><p>Feature of Aurora:</p><ul><li>Only send log record– The storage server can apply the log to page, so Aurora can just apply log without applying dirty page, which reduces the network workload</li><li>Only 4 Nodes required to make consensus</li></ul><h3>Quorum Scheme</h3><p>If we have:</p><ul><li>N Replicas<ul><li>W for Writers’ consensus to move</li><li>R for Readers’ consensus to move</li></ul></li><li>R + W = N +1, this makes sure W &amp; R will get least one overlap</li></ul><p><strong>Example:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">N = 3</span><br><span class="line">R = W = 2 or R=3, W = 1</span><br><span class="line">We can adjust speed of R or W by adjusting the number of them</span><br></pre></td></tr></table></figure><p>In Aurora, N = 6, W = 4, R =3</p><h3>Conclusion</h3><p>In a word, Aurora optimized data transportation type and used quorum write scheme which got 35 times speeds up compared with RDS’s MySQL.</p><h2>Extends</h2><p>You can find more detailed description of Aurora’s work flow in <a href="https://zhuanlan.zhihu.com/p/319806107">Amazon Aurora: 避免分布式一致性</a> and <a href="https://zhuanlan.zhihu.com/p/338582762">浅谈Amazon Aurora</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Aurora&lt;/h1&gt;

&lt;h2&gt;Introduce&lt;/h2&gt;

&lt;p&gt;Amazon Aurora is a relational database service for OLTP workloads offered as part of Amazon Web Serv</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Aurora" scheme="https://zjuytw.github.io/tags/Aurora/"/>
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
  </entry>
  
  <entry>
    <title>6.824 CRAQ lecture note</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-9.CRAQ/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-9.CRAQ/</id>
    <published>2021-08-09T09:08:08.068Z</published>
    <updated>2021-08-09T10:05:27.977Z</updated>
    
    <content type="html"><![CDATA[<h1>CRAQ</h1><h2>Introduce</h2><p>Start with awareness that Paxos, Raft’s consensus algorithms have bottleneck in leader, because leader need to send logs to all followers and wait for majority of their reply.</p><p>Chain replication is purposed in 2004, its scheme ensures serializability meanwhile the whole workload is distributed among the system(to each node).</p><p>CRAQ is an improvement on Chain Replication, maintains strong consistency while greatly improving read throughput. <strong>But in this article, I will just mainly talk about Chain Replication.</strong></p><h2>Chain Replication</h2><p><img src="/2021/08/09/6.824-9.CRAQ/CR.png" alt="CR"></p><p>Use the chain on Figure 1 and a remote coordinating cluster like ZK, Raft or Paxos to check heartbeat and send configurations to nodes on the chain.</p> <h3>Failure Recovery</h3><ul><li>Head fails: Its successor becomes head</li><li>Tail fails: Its predecessor becomes tail</li><li>Intermediate fails: predecessor send MSG to its successor</li></ul><h3> Evaluation </h3><ul><li>Pros:<ul><li>Head’s workload is far less than Raft’s leader’s. Because leader needs to send sync. packets and handle R/W log</li></ul></li><li>Cons:<ul><li>Every node can slow down the whole system. Raft, instead, just need a majority of nodes keep running.</li></ul></li></ul><h2> Extend</h2><p>To read more about CRAQ, you may find articles in <a href="https://zhuanlan.zhihu.com/p/344808961">浅谈Primary-Back Replication和Chain Replication</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;CRAQ&lt;/h1&gt;

&lt;h2&gt;Introduce&lt;/h2&gt;

&lt;p&gt;Start with awareness that Paxos, Raft’s consensus algorithms have bottleneck in leader, because leader</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
    <category term="CRAQ" scheme="https://zjuytw.github.io/tags/CRAQ/"/>
    
  </entry>
  
  <entry>
    <title>6.824 ZooKeeper lecture note</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-8.ZooKeeper/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-8.ZooKeeper/</id>
    <published>2021-08-09T09:07:17.945Z</published>
    <updated>2021-08-09T10:44:13.292Z</updated>
    
    <content type="html"><![CDATA[<h1>ZooKeeper(To be continuely updated)</h1><h2>Introduction</h2><p>ZooKeeper is a very popular service for coordinating processes of distributed applications, it provides a simple and high performance kernel for building more complex coordination primitives. We users can deploy ZK in many distributed applications like, services registration and discovery, cluster management as a coordinator.</p><p>ZooKeeper has the following features:</p><ul><li><p>Sequential Consistence</p><p> All client see the same data that one client’s transactions will applied on ZK in its original order</p></li><li><p>Atomicity</p></li><li><p>Single View</p><p>Clients see the same data no matter it talk to which server</p></li><li><p>High performance</p></li><li><p>High availability</p></li></ul><h2>Implementation</h2><p>The node in the tree is called <em>znode</em>, which stores data and node information. The main task for ZK is to coordinate but not file storage, so znode’s file size is smaller than 1MB</p><p><img src="/2021/08/09/6.824-8.ZooKeeper/ZooKeeper_namespace.png" alt="ZooKeeper namespace"></p><p>There are two types of znode</p><ul><li>Ephemeral : ZK will automated delete it, after session finishes</li><li>Persistent : Need client to delete explicitly</li></ul><h3>Node information</h3><p>Znode has a <em>sequential</em> flag, it will be issued a monotonically increased number  if flag is true when created, to mark the global sequential order of the znode. It also maintains a state information table call <em>Stat</em>.</p><p><img src="/2021/08/09/6.824-8.ZooKeeper/Znode_stat.png" alt="Znode stat"></p><h3>Sequential Consistency</h3><p>To achieve sequential consistency, ZK uses its own ZAB consensus algorithm, like Raft and Paxos in implementation but different in some details.</p><p>ZK guarantees the <strong>single client</strong> FIFO transactions order. For R/W, ZK has different rules</p><ul><li>For reads, Leader/Follower/Observer all can directly handle read request. (read locally)</li><li>For write, all writes requests need to send to leader then wait till reaching consensus.</li></ul><p><strong>Note:</strong> For those need read a fresh data, client may send a <sync> to leader, then send read to replica.</sync></p><h2> Conclusion</h2><p>This is a simply discussion about ZK, I am just dabble in distributed systems, so I will keep updating this article as my concept of ZK grows </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;ZooKeeper(To be continuely updated)&lt;/h1&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;ZooKeeper is a very popular service for coordinating processes of di</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Distributed Systems" scheme="https://zjuytw.github.io/tags/Distributed-Systems/"/>
    
    <category term="ZooKeeper" scheme="https://zjuytw.github.io/tags/ZooKeeper/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Lab4</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-lab4/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-lab4/</id>
    <published>2021-08-09T08:55:23.224Z</published>
    <updated>2021-08-09T10:47:25.908Z</updated>
    
    <content type="html"><![CDATA[<h1>Lab4</h1><h2> Overview </h2><p>Finally, we make it to the final lab, no doubt that it’s the most challenging work among 4 labs. In this part, we gonna implement two main K-V storage system, one is shardmaster and one is shardkv. The specific service of them will be discussed in FRAMEWORK part, let’s talk how much workload this lab may take first.</p><p>Compared with former labs, we have no paper to refer to  and no framework to follow. This lab emphasizes more on real producing environment that we need balance load (<strong>so shard comes</strong>) and make shard movable among muti-raft. So <em>shardmaster</em> is a configuration center that decides which raft group servers which shards, and <em>shardkv</em> is exactly one raft group that need to provide <em>PUT/APPEND/GET</em> service to their served shards. (I will put a figure in **conclusion **to make you easier to understanding)</p><h2> Lab4A</h2><p>Now we are to build shardmaster, which is a group consensus configuration center and records current serving raft groups and which shards one raft group servers. Our task is to adjust shards and dispatch every configuration it receives. The whole design frame is alike as <em>lab3</em>, so we can just copy lab3’s code and revise some part to satisfy the need of <strong>Rebalance Shards</strong> in each raft group.</p><p>So what we gonna do is:</p><ul><li>Build the basic K/V storage system based on lab3</li><li>Design a Shard Rebalance mechanism that meet the lab’s request that <em>The shardmaster should react by creating a new configuration that includes the new replica groups. The new configuration should divide the shards as evenly as possible among the full set of groups, and should move as few shards as possible to achieve that goal. The shardmaster should allow re-use of a GID if it’s not part of the current configuration</em></li><li>Implement <em>Join( ), Leave(), Move(), Query()</em> </li></ul><h3>Implementation</h3><p>The whole design is similar to <em>kvraft</em>, because we have implemented consistency and deduplicated underlayer, we can just mainly focus on <em>Join() &amp; Leave( )</em>. I just simply use the strategy that move one shard from largest group (which owns most shards) to smallest group. The boundary condition is:</p><ul><li>Group 0 should have no shards, which means once group 0 has shard, we should select it as largest group and no select it when its shards is least.</li><li>Every raft group’s shards disparity should &lt;= 1</li></ul><p>So let’s consider query() and move() first:  For query(<em>idx int</em>), just return <em>idx-th</em> index’s config and return latest config if <em>idx</em> == -1. <strong>Note : If idx exceeds the array length,  we should return a invalid config for upper layer’s double check rather than throwing a exception and exit</strong>. For <em>move()</em>, we just move one shard from one raft group to another. No more adjusting needed.</p><p>Then Join() and Leave(), these two function have both <em>Rebalance Shards</em> part which need we think carefully before coding. There is two allocation strategies: one is that I formerly described and another is <em>consistent hash</em>, it is a more realistic way to address <em>load balance</em>. (But I just use the straightforward way). Once we decided the <em>rebalance strategy</em>, what left is much easier that to handle Join(), we can just append a group has no shard then call <em>rebalance()</em>, to handle Leave(), we can just assign left group’s shards to group 0 then call rebalance().</p><p>Once one of <em>join, leave, move</em> is done, append a new config corresponding to it to <em>configs</em>.</p><h2> Lab4B</h2><p>This part takes me longest time to consider whole logic and the same time to debug. So I hope there are some problem to be addressed before programming.</p><p>First, what should one <em>shardKV</em> do. <em>shardKV</em> is a raft group which <em>shardMaster</em> previously recorded in config, to make you easier to understand, the normal format is like: “[server-101-0 server-101-1 server-101-2]” which means raft group 101 has 3 raft peers from 0 - 2. So these 3 servers is required to provide Key/Value service for their served shards. So simply speaking, if shards are statistic, a shardKV is just like kvraft.</p><p>Second, shards movement. Because shards are not fixed on one group, we should move shard from one group to another, so problem comes that <strong>which information should be delivered and how to make all group remains consistence.</strong></p><p>Third, which operations should be asynchronized in a group. If you haven’t have a figure of the whole frame in your head it’s OK, but let’s image first what we should do in this program. Start with configuration fetch, we should routinely check if there is a newer config then try to update. Then the migration method, it is unwise that once you need to get or send one shard’s data, you just stuck and wait for a reply. Third the GC thread, (optional)</p><p>Finally, how to achieve consistency on moving shards. It’s not hard to answer, we should just use leader to launch every request, use <em>rf.Start() to record</em>, once received command in <em>AppCh</em>, then start to process the command.</p><h3>Implementation</h3><p>To pass all test, we should consider Garbage Collection and Non-Stuck service when other shards are pulling. In whole program, we mainly implement ConfigFetch(), ShardMigration(), ShardGC(). All data structure and data stored is involved with these 3 main functions.</p><h4>Config Update</h4><p>As we discussed above, we will launch a thread to routinely fetch new config. Once we get a new config whose version number is <strong>exactly</strong> current number + 1, then we should set current config as new one. <strong>But here is the problem, how to deal with old data?</strong>  No matter we are gonna fetch shard from shrink group or push shard to escalated group, we both need to know the RPC address by <em>make_end(server_name string)</em>. So we need at least one previous config to send RPC call, **but do we need more previous config? ** If we apply every config once we detect a new one, it is likely we haven’t pull or push data before config change. So we need to save all previous config, and make sure one version’s shard data is all transported then we can recollect the memory. But it is much tremendous compared with another implementation I learned from <a href="http://tanxinyu.work/6-824/">Xinyu Tan’s Repo</a>that, we just wait till all group’s all shards are ready. So we can make sure that all group which apply new config are clear with previous shards’ data.</p><h4>ShardMigration</h4><p>For one shard move from one group to another, it is both OK that we can choose pull or push from either side. In my implementation, I choose to pull from shrink group. Now we can consider what data should be stored, like previous lab, we need both <em>kvtable &amp; ClerkRecord</em> to deduplicate. It is a general thought we can store these info. for every shard and migrate with shard. Also, we need a status id to notify current status. We need 4 status to mark different states.</p><ul><li>Serving (ready state)</li><li>Pulling (Config changes and this shard is prepared to serve after pulling data from previous one)</li><li>BePulling(Config changes and this shard is prepared to be pulled data by new group)</li><li>GCing (After fetch shard’s data, we need a new RPC call to tell previous one to delete shard)</li></ul><h4>ShardGC</h4><p>For those shards shrunk from previous config to current one, we are respond to recollect these memory. So we design a new mechanism to process GC, which means once new group successfully fetched shard data from previous one, (Pulling -&gt; GCing), there are a thread routinely check <em>GCing</em> state and send GC signal to other group to tell it, it’s OK to delete data.</p><h4> Hints</h4><ul><li><p>Every shard can receive request from client if they are in <em>Serving</em> and <em>GCing</em></p></li><li><p>All Thread should check if it’s leader, then begin to process formal job</p></li><li><p> To deal with unreliable network, we should use configure number to check if is the right request.</p></li><li><p>In GC step, the GCed group should first use Raft record the GC command then delete and send reply.(Or once this time it crashed, no one will notify it to GC again, because the current shard’s group has received the reply and turn GCing to Serving)</p></li><li><p>Time to detect <em>Wrong Group</em></p><ul><li>In previous design, we just exam <em>Sequence number</em>, but now we should do group exam to block those not serving shard’s request</li><li>It is not enough to exam it at first, just imagine a situation</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">putAndAppend</span><span class="params">(cmd *Op)</span></span>&#123;</span><br><span class="line">key, value, op, cid, seq := cmd.Key, cmd.Value, cmd.Op, cmd.Cid, cmd.Seq</span><br><span class="line">kv.mu.Lock()</span><br><span class="line"><span class="keyword">defer</span> kv.mu.Unlock()</span><br><span class="line"><span class="comment">//shard := key2shard(key)</span></span><br><span class="line"><span class="comment">//if ok := kv.checkShard(shard); !ok&#123;</span></span><br><span class="line"><span class="comment">//cmd.Err = ErrWrongGroup</span></span><br><span class="line"><span class="comment">//return</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line">ok := kv.updateClerkMap(cid, seq, shard)</span><br><span class="line"><span class="keyword">if</span> ok&#123;</span><br><span class="line">then ...</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    if we do not exam shard in <em>Put, Append, Get</em>, we may have inconsistency consequence</p><ul><li>Shard have updated(Because of asynchrony flow), and it is now not serving in this group. Data do have <em>Put, Append, Get</em> and clerkRecord updated. But it won’t be watched in new serving group</li></ul></li></ul><p>So we need a mechanism to tell previous Public interface (Put(), Append(), Get()) that our shards have changed, that there is an ErrWrongGroup out there. Thus I adapted a different way to watch progress of each operation, channel. That every operation gets its channel identified by the return index value of <em>rf.Start()</em>. So once the command is executed, the kvraft will notify interface that operation has been done by channel(as well as err).</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">getChan</span><span class="params">(raftIndex <span class="keyword">int</span>, create <span class="keyword">bool</span>)</span><span class="title">chan</span> <span class="title">Op</span></span>&#123;</span><br><span class="line">kv.mu.Lock()</span><br><span class="line"><span class="keyword">defer</span> kv.mu.Unlock()</span><br><span class="line"><span class="keyword">if</span> _,ok := kv.chanMap[raftIndex]; !ok&#123;</span><br><span class="line"><span class="keyword">if</span> !create&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line">kv.chanMap[raftIndex] = <span class="built_in">make</span>(<span class="keyword">chan</span> Op,<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> kv.chanMap[raftIndex]</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> kv.chanMap[raftIndex]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">listen</span><span class="params">(ch <span class="keyword">chan</span> Op, index <span class="keyword">int</span>)</span> <span class="title">Op</span></span>&#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> op,ok := &lt;-ch:</span><br><span class="line"><span class="keyword">if</span> ok&#123;</span><br><span class="line"><span class="built_in">close</span>(ch)</span><br><span class="line">&#125;</span><br><span class="line">kv.mu.Lock()</span><br><span class="line"><span class="built_in">delete</span>(kv.chanMap, index)</span><br><span class="line">kv.mu.Unlock()</span><br><span class="line"><span class="keyword">return</span> op</span><br><span class="line"><span class="keyword">case</span> &lt;- time.After(time.Duration(<span class="number">500</span>) * time.Millisecond):</span><br><span class="line"><span class="keyword">return</span> Op&#123;Err: ErrWrongLeader&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Test Result:</p><p><img src="/2021/08/09/6.824-lab4/Lab4SingleTest.png" alt="Lab4 SingleTest"></p><p>100 times tests result:<img src="/2021/08/09/6.824-lab4/lab4TestA.png" alt="lab4 TestA"></p><p><img src="/2021/08/09/6.824-lab4/lab4TestB.png" alt="lab4 TestB"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Lab4&lt;/h1&gt;

&lt;h2&gt; Overview &lt;/h2&gt;

&lt;p&gt;Finally, we make it to the final lab, no doubt that it’s the most challenging work among 4 labs. In t</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Lab Note" scheme="https://zjuytw.github.io/tags/Lab-Note/"/>
    
    <category term="ShardKV Storage" scheme="https://zjuytw.github.io/tags/ShardKV-Storage/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Lab4 Conclusion</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-lab4-Conclusion/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-lab4-Conclusion/</id>
    <published>2021-08-09T08:55:23.223Z</published>
    <updated>2021-08-09T10:06:50.016Z</updated>
    
    <content type="html"><![CDATA[<h1> Conclusion</h1><p>In this part, I will illustrate more detailed implementation of ShardKV based on the expectation that you have read the lab4 and know the basic idea of the framework. And I will describe following the sequence of a message sent by Clerk and to each part of Raft Group.</p><img src="/2021/08/09/6.824-lab4-Conclusion/structure3.png" alt="structure3" style="zoom: 150%;"><h2> Implementation</h2><h3>ShardKV</h3><p>As we described previously, we should maintain a Shard Map, config &amp; lastConfig information in each Raft Group to complete 3 main functions.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> ShardKV <span class="keyword">struct</span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    chanMap     <span class="keyword">map</span>[<span class="keyword">int</span>] <span class="keyword">chan</span> Op</span><br><span class="line">lastcfg     shardmaster.Config</span><br><span class="line">cfg         shardmaster.Config</span><br><span class="line">servedShard <span class="keyword">map</span>[<span class="keyword">int</span>]Shard</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Shard <span class="keyword">struct</span>&#123;</span><br><span class="line">    Table <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span></span><br><span class="line">ClerkRecord <span class="keyword">map</span>[<span class="keyword">int64</span>]<span class="keyword">int</span></span><br><span class="line">Status <span class="keyword">uint8</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3>Interface(Put, Append, Get) </h3><p>To receive ErrWrongGroup msg returned while true executing, we need to listen to each channel. We can allocate channel according to the Raft’s log index. What’s more, because of raft’s reelection, the map of index -&gt; channel is not unique, so we should also check whether the value channel returned is what you want.</p><h3> K/V table & ClerkRecord</h3><p>Just change the manipulating data from global map to each shard’s own map</p><h3> Config Update</h3><p>One of the threads watching for new configurations, and once finds a new confg, send a command to Raft to make a consensus </p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">pullConfig</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> !IsLeader() || !IsAllReady()&#123;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">nextcfg := kv.cfg.Num + <span class="number">1</span></span><br><span class="line">cfg := kv.mck.Query(nextcfg)</span><br><span class="line"><span class="keyword">if</span> nextcfg == cfg.Num&#123;</span><br><span class="line">        kv.rf.Start(cfg)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> Once applied a new configuration, we should figure out <em>MoveIn &amp;&amp; MoveOut</em>. For those <em>MoveIn</em>, turn status to <em>Pulling</em>, waiting for pulling data from previous group. As for <em>MoveOut</em>, turn status to <em>BePulling</em>, waiting for other’s pulling and GC.</p><p><strong>Note:</strong> Configuration update should be done after under layer rafts reach a agreement.</p><h3> Shard Update</h3><p>We should launch a thread to routinely pull shards whose status are “Pulling”. So we should design a RPC call arg and reply, We need Confignum to mark the sender’s version in args and Shard Num and ConfigNum in reply because we just simply put the reply into Raft to record command of <em>Update Shard</em> later.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> PullShardArgs <span class="keyword">struct</span>&#123;</span><br><span class="line">Shard  <span class="keyword">int</span></span><br><span class="line">ConfigNum <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> PullShardReply <span class="keyword">struct</span> &#123;</span><br><span class="line">Err   Err</span><br><span class="line">Shard <span class="keyword">int</span></span><br><span class="line">ConfigNum <span class="keyword">int</span></span><br><span class="line">Table <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span></span><br><span class="line">ClerkRecord <span class="keyword">map</span>[<span class="keyword">int64</span>]<span class="keyword">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">pullShard</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> !IsLeader()&#123;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">moveIn := kv.getShardsByStatus(Pulling)</span><br><span class="line"><span class="keyword">var</span> wait sync.WaitGroup</span><br><span class="line"><span class="keyword">for</span> _, shard := <span class="keyword">range</span> moveIn&#123;</span><br><span class="line">wait.Add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(shard <span class="keyword">int</span>,cfg shardmaster.Config)</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wait.Done()</span><br><span class="line">gid := cfg.Shards[shard]</span><br><span class="line">servers := <span class="built_in">make</span>([]<span class="keyword">string</span>, <span class="built_in">len</span>(cfg.Groups[gid]))</span><br><span class="line">currentCfgNum := cfg.Num + <span class="number">1</span></span><br><span class="line"><span class="built_in">copy</span>(servers, cfg.Groups[gid])</span><br><span class="line"><span class="keyword">for</span> i:= <span class="number">0</span>; i &lt; <span class="built_in">len</span>(servers); i++&#123;</span><br><span class="line">server := servers[i]</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">srv := kv.make_end(server)</span><br><span class="line">args := PullShardArgs&#123;</span><br><span class="line">Shard:     shard,</span><br><span class="line">ConfigNum: currentCfgNum,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> reply PullShardReply</span><br><span class="line">                    RPC call(&amp;args, &amp;reply)</span><br><span class="line">                    <span class="keyword">if</span> successful&#123;</span><br><span class="line">                        kv.rf.Start(reply)</span><br><span class="line">                    &#125;</span><br><span class="line">&#125;()</span><br><span class="line">&#125;</span><br><span class="line">&#125;(shard, kv.lastcfg)</span><br><span class="line">&#125;</span><br><span class="line">kv.mu.Unlock()</span><br><span class="line">wait.Wait()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>After receiving <em>PullShardReply</em> in <em>appCh</em>, we are gonna try to update shard.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> cmd.ConfigNum == kv.cfg.Num&#123;</span><br><span class="line">    table := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>)</span><br><span class="line">    cRecord := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">int64</span>]<span class="keyword">int</span>)</span><br><span class="line">    deepCopyTable(table, cmd.Table)</span><br><span class="line">    deepCopyRecord(cRecord, cmd.ClerkRecord)</span><br><span class="line">    <span class="keyword">if</span> kv.servedShard[cmd.Shard].Status == Pulling&#123;</span><br><span class="line">        kv.servedShard[cmd.Shard] = Shard&#123;</span><br><span class="line">            Table:       table,</span><br><span class="line">            ClerkRecord: cRecord,</span><br><span class="line">            Status:      GCing,</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3> Garbage Collection</h3><p>Once new shard receives its data from old one, it is responsible to send GC signal to old group to help it delete useless shard data. So we need a new RPC call type and a thread routinely check if there is shard status is <em>GCing</em>, if yes then send GC RPC shard by shard.</p><p><strong>Note:</strong> </p><ul><li>one trap here is, because of unreliable network, there may be a situation that after received <em>GC signal</em> old one has deleted the data but new shard doesn’t receive the reply. So old group move on, but new one remains sending GC but no reply.  Solution is, let GC() handler return OK whenever it receives an outdated ConfigNum request.</li><li>To delete shard, the old group should wait till Raft reaches a agreement then reply a OK message. Or due to lose of command in Raft, there may just one peer( the old leader ) deleted the shard</li></ul><h3>Snapshot </h3><p>Just keep your new data persist</p><h2> Some key points </h2><ul><li>Pay attention to reference copy, especially the map sending from Raft’s channel</li><li>A big trap here is, the whole system may stuck in a livelock due to restart. The situation is, because raft leader need a current Term’s log to update commit Index. But once there is a restart of raft, plus there happen no new command comes in. The raft won’t commit nothing, and because this group haven’t replayed to newest state, other group can not move on either( They count on the stuck raft to make GC or Pull date from the raft). <strong>And the solution is commit a blank command whenever a new leader is selected to help raft move on</strong></li><li>Believe you can finish this!!!</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt; Conclusion&lt;/h1&gt;

&lt;p&gt;In this part, I will illustrate more detailed implementation of ShardKV based on the expectation that you have read</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Lab Note" scheme="https://zjuytw.github.io/tags/Lab-Note/"/>
    
    <category term="ShardKV Storage" scheme="https://zjuytw.github.io/tags/ShardKV-Storage/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Conclusion</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-lab3-Conclusion/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-lab3-Conclusion/</id>
    <published>2021-08-09T08:53:11.992Z</published>
    <updated>2021-08-09T10:06:39.243Z</updated>
    
    <content type="html"><![CDATA[<h1>Conclusion</h1><p>Lab3 is a tough journey, especially when you finish the whole framework, then problem just starts. I took roughly two days to complete my draft and spent 2-3 times time to debug.</p><p>In order to remind me of this hard time, spending hours reading 20000 lines log, suspecting each line I wrote and make you who are reading this avoid bugs or debug smoothly, I will write problems as explicit as I can.</p><p>To make my design easy to understand, I will start with design graph.</p><h2> Structural Design</h2><p><img src="/2021/08/09/6.824-lab3-Conclusion/structural1.png" alt="structural1"></p><p><img src="/2021/08/09/6.824-lab3-Conclusion/structural2.jpg" alt="structural2"></p><h2> Specific Implementation</h2><h3> Overview</h3><p>Because detailed Raft design is described in former part, so let’s just assume we all pass Lab2 perfectly (Check it every time you modify Raft’s code!)</p><ul><li>For KV Server<ul><li>In 3A, we achieve the functions to send command and listen to channel till <em>Raft</em> peers reach agreement and persist the command, once <em>Server</em> receives specific message, it should execute the command(Without repetition).</li><li>In 3B, we will implement snapshot. This idea is straightforward but we need to overcome the side-effect it brings.</li></ul></li><li>For Raft<ul><li>We need not modify our Raft design in 3A</li><li>In 3B, we should modify our code to deal with what Snapshot brings<ul><li>the first problem is to make your <em>Raft</em> execute more fast to pass <strong>TestSnapshotSize3B</strong>. </li><li>What time to tell KVServer to snapshot</li><li>Index Convert</li></ul></li></ul></li></ul><h3>Concrete Plan</h3><h4>KVServer</h4><ul><li><p>To avoid repeating execution, assign each clerk an unique ID and mark each command a monotonically increasing sequence number. So <em>Server</em> can exam this info. which stored in <em>map</em> to decide whether execute.</p></li><li><p>KVServer:: Get()  &amp; PutAppend()</p><p>A tricky implementation here is, in Get() we can achieve faster response by check the ClerkID’s current sequence number to get value directly.</p><p>You can do timeout check by <strong>Receive a event of timer.After</strong> or <strong>Condition variable of check a count value</strong></p></li><li><p>KVServer:: commitCommandHandler()</p><p>A thread function to listen to appMsg Channel. If receives message, it will do <strong>Valid check, Index check and Command check</strong> to execute a command in right way.</p><p><strong>Note:</strong> Because the channel is a FIFO sequence, <em>Server</em>‘s execution sequence is decided by Raft’s push sequence. It’s a nice point that we don’t need to pay extra attention keep <em>Raft</em> and <em>KVServer</em> in sync, especially when <em>Raft</em> requires for a Snapshot in a very index.</p></li><li><p>appMsg Channel with buffer</p><ul><li>To make execution more efficient and decouple <em>Raft</em> and <em>KVServer</em>. We can preallocate a buffer for this channel so <em>Raft</em>‘s log can be pipelined down into channel.</li><li>**Note:**Don’t worry about inconsistency, <em>Server</em> will still execute in sequence and <em>Raft</em> will still get the right index’s <em>Server’s</em> state Snapshot.</li></ul></li></ul><h4>Raft</h4><ul><li>Remember to check your <em>Raft</em> first when you encounter a problem!!!(90% bug)</li></ul><p>I actually don’t wanna talk a lot about basic design here, for <strong>InstallSnpshot RPC</strong> is much similar as previous design. But there are still some point I want to record.</p><ul><li><p>To achieve high concurrency.</p><ul><li>In previous part, I just make <strong>HeartBeat &amp; EntriesAppend</strong> a same mechanism to send : goroutine check rf.log every 100ms and send new log if nextIndex[i] &lt;= len(log) - 1.</li><li>Now we can add an extra send action in Start( )</li><li><strong>Note:</strong> Be careful when you do so. Imagine a partition situation: a network-failed leader gonna send his log to others each time it receives new log. Once it’s reconnected and receives info. from current leader and is ready to become follower, but  <strong>sendAppendEntries</strong> has been called. So a packet with up-to-dated term and false log will be received and accepted by other followers.<ul><li>The problem is caused by checkStatus() step and sendAppendEntries() are not atomic, and in low concurrency situation this failure somehow are less likely to happen. </li></ul></li></ul></li><li><p>Time point of chitchat</p><ul><li><em>Server</em> should send <em>maxRaftState</em> to <em>Raft</em> during init(), each time Raft do a commit, it should check RaftStateSize and send signal if oversize.</li><li>Each time <em>Raft</em> truncates log, <em>lastApplied</em> should be changed as well as <em>IastIncludedIndex</em></li><li><strong>Note:</strong> I read other’s Blog and they all mentioned one problem of deadlock which happens between Server and Raft. In my design, we don’t need to worry about it, for each layer is working just at its own range. If you design a structure that Locks kv.mu and rf.mu in same time, you should be aware of the bad consequence it may cause.</li></ul></li><li><p>A weird slice reference problem happened on me</p><ul><li><p>Problem description: Leader has a log : [A, B, C, D, E] for short, and in one moment, the AppendEntries log it send became : [C, D, C, D, E]. I checked locks and run hundreds of time of race detection with race-free. It costs nearly 1 day to locate and find reason.</p></li><li><p>If you aren’t interested in this problem, just keep in mind: Golang’s slice has a implicit mechanism of copy and truncate, each time you use <strong>newdata = data[start :  end]</strong>, you have the idea that newdata will be effected as long as data changed. The bug will sometimes even make no sense when you do more operation on newdata.</p></li><li><p>Now let me intro my bug:</p><p><img src="/2021/08/09/6.824-lab3-Conclusion/bug1.png" alt="image-20210706133342852"></p><p>​                                                            Think about what Test will print</p><ul><li><p>It seems slice are not allocate <em>b</em> a new space, so <em>b’s</em> pointer is point to data’s physical array[2], what makes thing worse is</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="number">0</span>] = data[<span class="number">4</span>]</span><br><span class="line">tmp := data[<span class="number">5</span> : ]</span><br><span class="line">data = data[:<span class="number">1</span>]</span><br><span class="line">data = <span class="built_in">append</span>(data, tmp...)</span><br></pre></td></tr></table></figure><p>also not assign a new memory to data. </p><p><img src="/2021/08/09/6.824-lab3-Conclusion/bug2.png" alt="image-20210706133933221"></p><p>​                                                                  So this is the result</p></li><li><p>But you can just use </p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = data[<span class="number">4</span>:]</span><br></pre></td></tr></table></figure><p>to truncate your log, because it will allocate a new array to data.</p></li><li><p>Now let’s check my code</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Called by KVServer to set Snapshot*/</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">SetSnapshot</span><span class="params">(snapshot []<span class="keyword">byte</span>, index <span class="keyword">int</span>)</span></span>&#123;</span><br><span class="line">rf.mu.Lock()</span><br><span class="line"><span class="keyword">defer</span> rf.mu.Unlock()</span><br><span class="line"><span class="keyword">if</span> rf.lastIncludedIndex &gt;= index &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//rf.log[0] = rf.log[rf.logIndexToArrayIndex(index)]</span></span><br><span class="line"><span class="comment">//tmp := rf.log[rf.logIndexToArrayIndex(index+1):]</span></span><br><span class="line"><span class="comment">//rf.log = rf.log[:1]</span></span><br><span class="line"><span class="comment">//rf.log = append(rf.log, tmp...)</span></span><br><span class="line">rf.log = rf.log[rf.logIndexToArrayIndex(index) : ]</span><br><span class="line">rf.lastIncludedIndex =  index</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">Be careful, the lastApplied need to be updated as well</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">rf.lastApplied = index</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">startAppendEntries</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">//c := time.Now()</span></span><br><span class="line">rf.mu.Lock()</span><br><span class="line"><span class="keyword">if</span> rf.status != LEADER&#123;</span><br><span class="line">rf.mu.Unlock()</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">index := <span class="built_in">len</span>(rf.log) - <span class="number">1</span> + rf.lastIncludedIndex</span><br><span class="line">term := rf.currentTerm</span><br><span class="line">leaderid := rf.me</span><br><span class="line">leaderCommit := rf.commitIndex</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(rf.peers); i++ &#123;</span><br><span class="line"><span class="keyword">var</span> logs []Pair</span><br><span class="line"><span class="keyword">var</span> prevLogIndex, prevLogTerm <span class="keyword">int</span></span><br><span class="line"><span class="keyword">if</span> i == leaderid &#123;</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> rf.nextIndex[i] &lt;= rf.lastIncludedIndex&#123;</span><br><span class="line"><span class="comment">//gonna send snapshot</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(i <span class="keyword">int</span>, lastIncludedIndex <span class="keyword">int</span>, lastIncludedTerm <span class="keyword">int</span>, data []<span class="keyword">byte</span>)</span></span>&#123;</span><br><span class="line">                <span class="comment">/* Install Snapshot RPC*/</span></span><br><span class="line">...</span><br><span class="line">&#125;(i, rf.lastIncludedIndex, rf.log[<span class="number">0</span>].Term, rf.persister.ReadSnapshot())</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> index &lt; rf.nextIndex[i] &#123;</span><br><span class="line">prevLogIndex = index</span><br><span class="line">prevLogTerm = rf.log[rf.logIndexToArrayIndex(prevLogIndex)].Term</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> index &gt;= rf.nextIndex[i] &#123;</span><br><span class="line">logs = rf.log[rf.logIndexToArrayIndex(rf.nextIndex[i]) : rf.logIndexToArrayIndex(index+<span class="number">1</span>)]</span><br><span class="line">prevLogIndex = rf.nextIndex[i] - <span class="number">1</span></span><br><span class="line">prevLogTerm = rf.log[rf.logIndexToArrayIndex(prevLogIndex)].Term</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(i <span class="keyword">int</span>, prevTerm <span class="keyword">int</span>, prevIndex <span class="keyword">int</span>, logs []Pair)</span></span> &#123;</span><br><span class="line">args := AppendEntriesArgs&#123;</span><br><span class="line">Term:         term,</span><br><span class="line">LeaderId:     leaderid,</span><br><span class="line">PrevLogIndex: prevIndex,</span><br><span class="line">PrevLogTerm:  prevTerm,</span><br><span class="line">Entries:      logs,</span><br><span class="line">LeaderCommit: leaderCommit,</span><br><span class="line">&#125;</span><br><span class="line">reply := AppendEntriesReply&#123;&#125;</span><br><span class="line">ok := rf.sendAppendEntries(i, &amp;args, &amp;reply)</span><br><span class="line">rf.mu.Lock()</span><br><span class="line">            ...</span><br><span class="line">&#125;</span><br><span class="line">rf.mu.Unlock()</span><br><span class="line">&#125;(i, prevLogTerm, prevLogIndex, logs)</span><br><span class="line">&#125;</span><br><span class="line">rf.mu.Unlock()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>I just implicitly use the reference of log and pass it to the closure to call RPC. Although I lock the whole body of <em>startAppendEntries()</em>, the context within the closure is not atomic, so left log a time window to make change.</p></li></ul></li></ul></li><li><p>When you encounter with other bugs: DPrint is a good helper  to locate errors, and make sure every variable  goes on track.</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Lab3 is a tough journey, especially when you finish the whole framework, then problem just starts. I took roughly tw</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="KV Raft" scheme="https://zjuytw.github.io/tags/KV-Raft/"/>
    
    <category term="Lab Note" scheme="https://zjuytw.github.io/tags/Lab-Note/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Lab3 KVRaft</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-Lab3/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-Lab3/</id>
    <published>2021-08-09T08:53:03.692Z</published>
    <updated>2021-08-09T10:06:36.028Z</updated>
    
    <content type="html"><![CDATA[<h1> Lab3 </h1><h2>Overview</h2><p>In this Lab, we are gonna build a Key/Value storage system based on raft, AKA KVRaft. In part A, just maintain a in memory Hashmap of Key -&gt; Value and In part B, we need to deal with the growing Hashmap and use Snapshot to discard old log entries. Some details may be discussed in each section.</p><h2>Lab3A</h2><p>To implement a KVRaft, we use <em>clerk</em> to denote a interface the client calls to use K/V storage, <em>server</em> to denote  a peer among  K/V storage system which is based on Raft protocol. </p><p>Particularly, we need to address of some detailed problems</p><ul><li>Leader find<ul><li>Whether make server report to clerk which is leader(Not applicable for this lab)</li></ul></li><li>Repeat request<ul><li>Like, send <em>append</em> request to a raft while due to the unreliable RPC, before receiving a commit server receives a “timeout”. So when you are trying append secondly, you should  be careful not to append twice.</li></ul></li><li>Be careful of deadlock</li></ul><h3>Implementation</h3><ul><li><p>To find leader, clerk just use randomized generate server_id to query for true leader, and once get positive response <em>clerk</em> will record this id for later use. </p><p>Furthermore, I want to explain why server not send a leader_id to clerk. In this lab3a,  the server list every clerk maintains are different as I <em>Dprint(ck.leader)</em> shows, which means there are not a standard leader_id for all clerk</p></li><li><p>Generate a random id for each clerk and mark each operation a monotonic increasing sequence number to erase repeat.</p><p>The erase repeating step should be processed in <em>server</em> level, not in <em>raft</em> level. </p><p>For these out-dated operation, because the sequence number for each clerk is monotonically increasing, server just skip manipulating the data but instead return positive</p></li></ul><p><img src="/2021/08/09/6.824-Lab3/Test3A.png" alt="Test3A"></p><h2>Lab3B</h2><p>This time, we are gonna add snapshot feature to make raft discard old log entries. It requires us to modify our original Raft design and implement some new functions and <em>InstallSnapshot</em>.</p><p>In my structure,  KV server will tell Raft <em>maxRaftSize</em> , so when log’s size is greater than this, Raft will notify server and require for a snaphot for <em>current_Index</em> (a tricky point is, because KV server receive msg from channel one after another, there won’t be concurrency problem). And once KV server send a snapshot to Raft, Raft just save it and discard old entries.</p><p>When crash happens, Raft should send the newest snapshot to server.</p><h3> Implementation</h3><ul><li>Interface design<ul><li>Raft :: SendMaxRaftState(), (To be called by Server)</li><li>Raft :: applyMsg{} :: Command -&gt; “Require Snapshot”</li><li>Raft :: SendSnapshot(), (To be called by Server to send snapshot to Raft)</li><li>Raft :: applyMsg{} :: Command -&gt; “Commit Snapshot”</li></ul></li><li> Index convert</li><li>To discard old log,  we should keep <strong>last Snapshoted Index</strong>, I use <em>lastIncludedIndex</em> to denote it. When we do Snapshot or Install Snapshot, we all need reset <em>lastIncludedIndex</em></li><li>Do index convert modification in each code uses rf.log</li><li>Usage of snapshot<ul><li>when Raft crashed and reboots, it should send KV server its snapshot and restart at <em>lastIncluded</em> index</li><li>if a leader detects one follower’s nextindex falls too behind and its nextIndex is lower than truncated leader’s index, Leader should send it a snapshot.</li><li>Every snapshot is accompanied with truncated index, if receiver’s <em>lastIncludedIndex</em> is larger than the truncated index, then just return</li></ul></li><li>Some detailed hints<ul><li>Raft need persist <em>lastInculudedIndex</em> just for restart use. <strong>When lastIncludedIndex changed, lastApplied need to be updated as well</strong></li><li>Chitchat point between KVServer and Raft.  <ul><li>Raft will send appMsg to KVServer to keep in synch. overall</li><li>To send snapshot, KVServer calls Raft’s public interface</li><li>I will describe these in detail in <strong>Conclusion</strong>.md</li></ul></li></ul></li></ul><p>Test Results:</p><p><img src="/2021/08/09/6.824-Lab3/Lab3SingleTest.png" alt="Lab3SingleTest"></p><p>100 times batch test:</p><p><img src="/2021/08/09/6.824-Lab3/Lab3BTest.png" alt="Lab3BTest"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt; Lab3 &lt;/h1&gt;

&lt;h2&gt;Overview&lt;/h2&gt;

&lt;p&gt;In this Lab, we are gonna build a Key/Value storage system based on raft, AKA KVRaft. In part A, just</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="KV Raft" scheme="https://zjuytw.github.io/tags/KV-Raft/"/>
    
    <category term="Lab Note" scheme="https://zjuytw.github.io/tags/Lab-Note/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Lab2 Conclusion</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-lab2-Conclusion/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-lab2-Conclusion/</id>
    <published>2021-08-09T08:41:55.508Z</published>
    <updated>2021-08-09T10:03:38.200Z</updated>
    
    <content type="html"><![CDATA[<h1>Conclusion</h1>After finish 3 parts of Lab, I have implemented a basic raft protocol and it can used to build a  K-V storage server. It is fascinating for its understandability and straight idea to build this consensus system. In past 3 parts, I achieved timeout election, append entries, persist and many subtle details to ensure the consistency. My implementation may seem somehow ungraceful especially when I looked the tutorial course 'Go thread and Raft', so I do recommend you to have a look at it before you start your first line of code.<table><thead><tr><th align="center">storage server 1</th><th align="center">storage server 2</th><th align="center">storage server 3</th></tr></thead><tbody><tr><td align="center">K/V service</td><td align="center">K/V service</td><td align="center">K/V service</td></tr><tr><td align="center">Raft</td><td align="center">Raft</td><td align="center">Raft</td></tr></tbody></table><p>In this distributed storage system, clients talk to application layer like K/V service, and application send <command> info to Raft to make distribute replicas. <em>Usually, Raft is a library to be included and easy used</em> </p><h3>    Features of Raft</h3><ul><li><p>Fault Tolerance(using 2n + 1 servers)</p><ul><li><p>To avoid <strong>split brain</strong>, adapt majority voting strategy. The basic logic is if there is a split brain, can not have both partitions own majority server. </p><p>In Raft’s detail, it means that for new leader, it will assemble at least one server’s vote which from previous majority. We should use one of those previous majority servers as new leader because we should ensure new leader must know what happened before(What term is before). </p></li></ul></li></ul><h2>TODO (ALL DONE)</h2> <p><del>I still have some issues need to be addressed, some are performance related and some are many bugs(I don’t know for sure, for me, I’d rather blame these FAIL to too strict test time restriction…)</del>  </p><ul><li>Some time, many 1 out of 10 the test will fail, for the program can not reach agreement. And the reason is easy, timeout election can sometime spend much time to elect a leader. And followings are solutions, from my point.<ul><li>Check if two peer are racing like, <em>one elected in Term x but in Term x + 1, another timeout and start a new round election</em>. For this situation, consider if timer triggers before you reset it</li><li>Sleep time is a mysterious number I’ve changed them for many times but still have some problem… Currently a fair sleep time is [200, 400] for heartbeat timeout and <del>[50, 100] for election timeout</del> </li><li>Some threads’ synchronization is implemented heavily so need a more soft way to achieve it(As I will describe afterwards)</li></ul></li><li>Some terrible codes need refactoring like substitute all channel by using condition variable and merge analogous code into one function. Especially, primary should become leader as long as it receives majority votes, not until all RPC calls return.</li></ul><p>Now I have revised my Raft and uses <a href="https://gist.github.com/jonhoo/f686cacb4b9fe716d5aa">Test Script</a> to test Lab 2 A + B + C 90 times, and result is passed all 90 tests.(I revised my code and continued my test from 10th test, <em>2 failed</em> is past recorded)</p><p><img src="/2021/08/09/6.824-lab2-Conclusion/Test.png" alt="Test"></p><p>This time, I have some new things to write about</p><ul><li>In fact, we do not need to set extract timer for RequestVote RPC, I once set [50,100]ms range to wait for RPC return, but test result turned out not so well. What we actually need to do is reset timer before start a new election round (So if RPC not return on time, timer will just start next election)</li><li>Apply interval time need to be considered carefully, because I once set 1000ms to apply new log to state machine, result are worse than 200ms’</li><li>When you are encountered with a livelock, check your RequestVote handler. I implemented this function mistakenly and led to the follower with up-to-data log cannot win the election on time. (Specifically, you need to make sure your peer convert to follower whenever it receives a RPC whose term is greater than currentTerm)</li><li>Check if your output channels are blocked and without receivers. Just uses channel with buffer(<strong>Can anybody tell me if go can GC unused channel buffer after both sender and receiver quit?</strong>)</li><li>Oh, I finally did not revise my election part using condition variable, just used channel.</li></ul><p><img src="/2021/08/09/6.824-lab2-Conclusion/Test2.png" alt="Test2"></p><p>​                                                            Result of a new 100 tests</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Conclusion&lt;/h1&gt;
After finish 3 parts of Lab, I have implemented a basic raft protocol and it can used to build a  K-V storage server. It</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Lab Note" scheme="https://zjuytw.github.io/tags/Lab-Note/"/>
    
    <category term="Raft" scheme="https://zjuytw.github.io/tags/Raft/"/>
    
  </entry>
  
  <entry>
    <title>6.824 Lab1 MapReduce</title>
    <link href="https://zjuytw.github.io/2021/08/09/6.824-lab1-MapReduce/"/>
    <id>https://zjuytw.github.io/2021/08/09/6.824-lab1-MapReduce/</id>
    <published>2021-08-09T08:40:40.263Z</published>
    <updated>2021-08-09T10:06:23.719Z</updated>
    
    <content type="html"><![CDATA[<h1> MapReduce </h1><h2>Summary</h2><h3>Topics about Distributed Systems </h3><ul><li><p>Infrastructure</p><ul><li>Storage</li><li>Communication</li><li>Computation</li></ul></li><li><p>Performance</p><ul><li>Scalability</li></ul></li><li><p>Fault Tolerance</p></li><li><p>Consistency</p><ul><li>strong consistency</li><li>weak consistency</li></ul></li><li><p>MapReduce</p></li></ul><h3> About MapReduce<p>TL;DR – Google’s engineers build a model that implements the split and dispatch process to make large scale computation tasks easy programmed. (Just need to write map() and reduce() function)</p><p><img src="/2021/08/09/6.824-lab1-MapReduce/MapReduce.png" alt="MapReduce"></p><p>​    </p><p>You can find the detailed model description in <strong>MapReduce: Simplified Data Processing on Large Clusters</strong></p><h4>Brief Execution flow<ol><li>Split the <em>input files</em> into M splits, which is generally 16-64 MB in size. And they are the basic unit one worker handles</li><li>We set up a <strong>master</strong> process to schedule the workflows of each worker, it will pass the <em>split</em> to idle worker (In fact the true process should be worker calls the master for a job)</li><li>Worker do their Map() job, and  the output of Map() should be &lt;key, value&gt; pair. </li><li> Store the <em>Intermediate output</em> on local disks (like json,etc).  AND!! Here is a trick that usually we need have R final reduced files, so we have to partition these &lt;key,value&gt; pairs into R parts. (Use hash(key) <strong>mod</strong> R)</li><li>Master received the <em>intermediate files</em> locations and once all the input files are processed. Master will assign Reduce() job to workers</li><li>Workers do the Reduce() job and output the final result on disk</li></ol><p>​    </p><h3>Some Hints on Lab1</h3><p>Claim: Below is all my own solution, and maybe happen to pass all tests. :)</p><ul><li><p>Enviornment Requirement</p><ul><li>GOLANG, <strong>You shall have basic golang knowledge like slice, map ‘s insert, delete, find… Also should know a little bit concurrency programming(just know what mutex mean hah), RPC programming…</strong></li><li>linux 64bit</li><li>For Chinese mainland students, if you are using vscode, you may have trouble install golang extensions in vscode. <strong>Google the problem, CSDN sucks</strong></li></ul></li><li><p>About master.go</p><ul><li>Checked input files <strong>map</strong> when a worker calls for job, also applicable for reduce job</li><li><strong>Recovery Problem</strong>: in master, I run a timecount gorountine for each assigned job, if the worker does not report on time, just modify todo list and assign it to the later worker( of course the former worker’s job is abandoned) </li><li>If all map tasks are assigned, but not completed, just tell idle workers WAIT.</li><li>If reduced tasks are all done, tell idle workers QUIT</li><li>Don’t forget to lock up the critical area</li></ul></li><li><p>About worker.go</p><ul><li>use a loop to constantly getJob, and if master tells worker QUIT,  then quit(Also WAIT, then wait)</li><li>Because each worker just read the seperate file, there is no synchronize problem when read</li><li>But for these worker on write, there is chance that abandoned worker is write the same file alone with new assigned worker<strong>You should be careful when write the intermediate/output file</strong></li></ul></li><li><p>About lambda function with goroutine</p><p><img src="/2021/08/09/6.824-lab1-MapReduce/exampleCode.png" alt="exampleCode"></p><p>​                                             Use my goroutine function for example*</p><p>When write lamda function, there is a trick that <strong>you should write these variable which you don’t want them be changed outside the lambda funtion as function’s parameter</strong> This is because that compiler will put the local variable(these used in lambda function) on heap NOT STACK, so once the variable is changed outside, the thread will read the changed variable. This is sometimes you don’t wanna see. </p></li></ul></h4></h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt; MapReduce &lt;/h1&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;h3&gt;Topics about Distributed Systems &lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Infrastructure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Storage&lt;/li&gt;
&lt;li</summary>
      
    
    
    
    <category term="6.824" scheme="https://zjuytw.github.io/categories/6-824/"/>
    
    
    <category term="Lab Note" scheme="https://zjuytw.github.io/tags/Lab-Note/"/>
    
    <category term="MapReduce" scheme="https://zjuytw.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://zjuytw.github.io/2021/08/09/hello-world/"/>
    <id>https://zjuytw.github.io/2021/08/09/hello-world/</id>
    <published>2021-08-09T03:10:31.995Z</published>
    <updated>2021-08-09T04:07:37.526Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>5841. Find the Longest Valid Obstacle Course at Each Position</title>
    <link href="https://zjuytw.github.io/2021/08/09/5841.%20Find%20the%20Longest%20Valid%20Obstacle%20Course%20at%20Each%20Position/"/>
    <id>https://zjuytw.github.io/2021/08/09/5841.%20Find%20the%20Longest%20Valid%20Obstacle%20Course%20at%20Each%20Position/</id>
    <published>2021-08-08T17:04:57.677Z</published>
    <updated>2021-08-09T08:34:33.477Z</updated>
    
    <content type="html"><![CDATA[<h1>5841. Find the Longest Valid Obstacle Course at Each Position</h1><p>tag: monotone-stack, greedy, Maximum Increasing Subsequence</p><h2>Description</h2><p><img src="/2021/08/09/5841.%20Find%20the%20Longest%20Valid%20Obstacle%20Course%20at%20Each%20Position/5841-1.png" alt="5841-1"></p><p><img src="/2021/08/09/5841.%20Find%20the%20Longest%20Valid%20Obstacle%20Course%20at%20Each%20Position/5841-2.png" alt="5841-2"></p><h2>Solution</h2><p>Use monotone-stack to find the longest subsequence end with <code>obstacles[i]</code></p><p>Greedily replace the very <code>obstacles[j], j &lt; i</code> that exactly greater than <code>obstacles[i]</code>, other elements in the stack just remain</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 3 5 7 4</span><br><span class="line"></span><br><span class="line">stack : 1 3 5 7</span><br><span class="line">after : 1 3 4 7</span><br></pre></td></tr></table></figure><p><strong>Just be careful about the same number should also be inclueded, so just binary search for (obstacle[i] + 1)</strong></p><h2>Code</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">longestObstacleCourseAtEachPosition</span><span class="params">(vector&lt;<span class="keyword">int</span>&gt;&amp; obstacles)</span> </span>&#123;</span><br><span class="line">        vector&lt;<span class="keyword">int</span>&gt; dp;</span><br><span class="line">        vector&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; obstacles.<span class="built_in">size</span>(); i++)&#123;</span><br><span class="line">            <span class="keyword">auto</span> pos = <span class="built_in">lower_bound</span>(dp.<span class="built_in">begin</span>(), dp.<span class="built_in">end</span>(), obstacles[i]+<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span>(pos != dp.<span class="built_in">end</span>())&#123;</span><br><span class="line">                *pos = obstacles[i];</span><br><span class="line">                res.<span class="built_in">push_back</span>(pos - dp.<span class="built_in">begin</span>() + <span class="number">1</span>);</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                dp.<span class="built_in">push_back</span>(obstacles[i]);</span><br><span class="line">                res.<span class="built_in">push_back</span>(dp.<span class="built_in">size</span>());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><img src="/2021/08/09/5841.%20Find%20the%20Longest%20Valid%20Obstacle%20Course%20at%20Each%20Position/5841-3.png" alt="5841-3"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;5841. Find the Longest Valid Obstacle Course at Each Position&lt;/h1&gt;

&lt;p&gt;tag: monotone-stack, greedy, Maximum Increasing Subsequence&lt;/p&gt;
&lt;</summary>
      
    
    
    
    <category term="Leetcode" scheme="https://zjuytw.github.io/categories/Leetcode/"/>
    
    
    <category term="monotone-stack" scheme="https://zjuytw.github.io/tags/monotone-stack/"/>
    
    <category term="greedy" scheme="https://zjuytw.github.io/tags/greedy/"/>
    
    <category term="maximum increasing subsequence" scheme="https://zjuytw.github.io/tags/maximum-increasing-subsequence/"/>
    
  </entry>
  
  <entry>
    <title>5840. Minimum Number of Swaps to Make the String Balanced</title>
    <link href="https://zjuytw.github.io/2021/08/09/5840.%20Minimum%20Number%20of%20Swaps%20to%20Make%20the%20String%20Balanced/"/>
    <id>https://zjuytw.github.io/2021/08/09/5840.%20Minimum%20Number%20of%20Swaps%20to%20Make%20the%20String%20Balanced/</id>
    <published>2021-08-08T17:00:27.810Z</published>
    <updated>2021-08-09T08:34:33.476Z</updated>
    
    <content type="html"><![CDATA[<h1>5840. Minimum Number of Swaps to Make the String Balanced</h1><p>tag: stack, greedy</p><h2>Description</h2><p><img src="/2021/08/09/5840.%20Minimum%20Number%20of%20Swaps%20to%20Make%20the%20String%20Balanced/5840-1.png" alt="5840-1"></p><p><img src="/2021/08/09/5840.%20Minimum%20Number%20of%20Swaps%20to%20Make%20the%20String%20Balanced/5840-2.png" alt="5840-2"></p><h2>Solution</h2><p>In each switch, brackets are reduced mostly 2, at least 1.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//Just swap the first dismatched ] with second dismatched [</span><br><span class="line">2 for: ]]] [[[ -&gt; []] [][. </span><br><span class="line">1 for just 1 pair left, switch them then all done</span><br></pre></td></tr></table></figure><h2>Code</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">minSwaps</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//只要[的右边有对应个数个]即可</span></span><br><span class="line">        stack&lt;<span class="keyword">int</span>&gt; stk;</span><br><span class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.<span class="built_in">size</span>(); i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(s[i] == <span class="string">&#x27;[&#x27;</span>)&#123;</span><br><span class="line">                stk.<span class="built_in">push</span>(i);</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">if</span>(stk.<span class="built_in">empty</span>())</span><br><span class="line">                    res++;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    stk.<span class="built_in">pop</span>();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res - res/<span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><img src="/2021/08/09/5840.%20Minimum%20Number%20of%20Swaps%20to%20Make%20the%20String%20Balanced/5840-3.png" alt="5840-3"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;5840. Minimum Number of Swaps to Make the String Balanced&lt;/h1&gt;

&lt;p&gt;tag: stack, greedy&lt;/p&gt;
&lt;h2&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/2021/08/09</summary>
      
    
    
    
    <category term="Leetcode" scheme="https://zjuytw.github.io/categories/Leetcode/"/>
    
    
    <category term="greedy" scheme="https://zjuytw.github.io/tags/greedy/"/>
    
    <category term="stack" scheme="https://zjuytw.github.io/tags/stack/"/>
    
  </entry>
  
</feed>
