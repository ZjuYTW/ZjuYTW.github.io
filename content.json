{"meta":{"title":"Yitao's Blog","subtitle":"","description":"","author":"Wang Yitao","url":"https://ZjuYTW.github.io","root":"/"},"pages":[{"title":"about","date":"2021-08-09T06:12:37.000Z","updated":"2021-08-09T06:12:37.914Z","comments":true,"path":"about/index.html","permalink":"https://zjuytw.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2021-08-09T06:35:31.000Z","updated":"2021-08-09T06:35:31.168Z","comments":true,"path":"categories/index.html","permalink":"https://zjuytw.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"6.824 Bitcoin lecture note","slug":"6.824-19. Bitcoin","date":"2021-08-09T09:13:23.361Z","updated":"2021-08-09T10:06:13.523Z","comments":true,"path":"2021/08/09/6.824-19. Bitcoin/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-19.%20Bitcoin/","excerpt":"","text":"Bitcoin As mentioned in Satoshi Nakamoto’s paper that bitcoin is aimed to prevent double-spending as well as reduce the cost of third party involvement. Bitcoin has three features that makes it Epoch-making Decentralization Using Peer-to-Peer Technology Low-cost transaction Brief Intro Bitcoin is a distributed ledger which implement decentralization. Bitcoin ‘s design should solve Byzantine Generals Problem because it is running through public Internet. Bitcoin systems reply on Proof of Work to verify each node’s validation to prove itself running on a true CPU. Bitcoin also promise that : Malicious nodes’ blockchain won’t grow long, if most of nodes in the network are meritorious. For double-spending problem, block-chain ensures that even if blockchain may fork at some point, but only one fork will be accepted in the end. Drawbacks Every new transaction need 10 min before recording on the blockchain The trustworthiness grows as the chain grows longer, but still have chance to be waived by receiving a longer chain from other node. Waste energy Extends You may read paper and watch lecture to get more detailed information about bitcoin.","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"Bitcoin","slug":"Bitcoin","permalink":"https://zjuytw.github.io/tags/Bitcoin/"}]},{"title":"6.824 CT lecture note","slug":"6.824-18. Certificate Transparency(Fork Consistency)","date":"2021-08-09T09:13:23.356Z","updated":"2021-08-09T10:45:57.253Z","comments":true,"path":"2021/08/09/6.824-18. Certificate Transparency(Fork Consistency)/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-18.%20Certificate%20Transparency(Fork%20Consistency)/","excerpt":"","text":"Certificate Transparency Introduce First review the vulnerability of HTTP and an example of Man in the Middle Attack. HTTP HTTP is a request response protocol to communicate asynchronously between client and server. For websites and pages the browser acts as a client and a web-server like Apache or IIS acts as server. The server hosts the files (like html , audio , video files etc) and returns are responds to client requests with the data. Depending on the request a response contains the status of the request. The process involves a series of messages that go back and forth between the client and server. The process starts with initiating a connection. After that a process known as TCP slow start kicks in. At this point data is passed between the two parties via data packets and often requires multiple round trips. TCP slow start is designed to gradually expand the amount of data traversing the wire each round trip. The initial packet size is 16kb and doubles on subsequent round trips until a max size is reached. This can vary, but tends to be around 4MB for most connections. This process is used because the server does not know how much bandwidth the client can handle. Rather than overflowing the client the server uses a gentle size and continues to increase until a limit is found. As data or request bodies move between the client and the server it is done using clear or plain text. This is viewable by anyone or software watching the network traffic. This is not that important for general content. However, today even when you don’t think sensitive data is moving between the two parties more sessions do transport identifying information. This is why every website should use TLS to secure HTTP connections. Man in the Middle Attack A third-party may easily hijack the connection towards target website and redirect to its own rogue web, for no further check mechanism in HTTP. Certificate, SSL, TLS, HTTPS HTTPS work flow: Particularly, server should request a certificate from CA(Certificate Authority). Whenever client send a connection request to server, it will receive a CERT from server. Certificate Server name, eg: “gmail.com” Public Key of server CA’s Signature CA’s unique signature ensures that just CA can issue the certificate to server that no one else can counterfeit. NOTE: A vulnerability of this scheme is that once a CA was broken into or something else happened and caused CA issued a malicious certificate. Client may have chance to talk to a rogue web and info may get leaked. Certificate Transparency Certificate Transparency is a system that stores certificate logs which are stored distributed and append-only on CT. CT can provide user certificate verification to keep CA from issuing malicious certificate and the certificate even keep in CA for longer time. CT promises following: Certificates are deposited in public, transparent logs Logs are cryptographically monitored Implementation Each certificates are stored as a node in Merkle Tree in CT. Each node in each level is the value of the output of cryptographic hash function that maps an arbitrary-size message M to a small fixed-size output H(M), with the property that it is infeasible in practice to produce any pair of distinct messages M1 ≠ M2 with identical hashes H(M1) = H(M2). And the we have 12h(0, K) = H(record K)h(L+1, K) = H(h(L, 2 K), h(L, 2 K+1)) With the property of above, we can determine whether a specific certificate stored in the tree, we can recompute hash(the certificate) and the hash value of its siblings and relatives to finally get the top-level’s hash. If H(4,0) == recomputed H(4,0), then proved. Example: For example, suppose we want to prove that a certain bit string B is in fact record 9 in a tree of 16 records with top-level hash T. We can provide those bits along with the other hash inputs needed to reconstruct the overall tree hash using those bits. Specifically, the client can derive as well as we can that: 1234567T = h(4, 0)= H(h(3, 0), h(3, 1))= H(h(3, 0), H(h(2, 2), h(2, 3)))= H(h(3, 0), H(H(h(1, 4), h(1, 5)), h(2, 3)))= H(h(3, 0), H(H(H(h(0, 8), h(0, 9)), h(1, 5)), h(2, 3)))= H(h(3, 0), H(H(H(h(0, 8), H(record 9)), h(1, 5)), h(2, 3)))= H(h(3, 0), H(H(H(h(0, 8), H(B)), h(1, 5)), h(2, 3))) Fork Attack (Fork Consistency) The proof of fork consistency, image a log servers have a chain of logs and once the log server wants to fork (like, to trick a user for a malicious certificate but not seen by other monitors etc.) CT has a mechanism to detect the inconsistency by gossip Like the example, once a log server has a fork line that starts with B for bogus and have a current STH(signed tree head, the top-level hash value). We can simply calculate if STH1’s log a prefix log of STH2’s log by the same way prove if STH1’s log is inside STH2’s log tree. If return a false, which means STH1 is on a different fork.","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"Certificate Transparency","slug":"Certificate-Transparency","permalink":"https://zjuytw.github.io/tags/Certificate-Transparency/"}]},{"title":"6.824 COPS lecture note","slug":"6.824-17.COPS(Causal Consistency)","date":"2021-08-09T09:13:23.348Z","updated":"2021-08-09T10:06:00.232Z","comments":true,"path":"2021/08/09/6.824-17.COPS(Causal Consistency)/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-17.COPS(Causal%20Consistency)/","excerpt":"","text":"COPS Introduce Still the website Front-End read from DB storage cluster model. We are gonna explore another possibility as local read and local write to speed up transactions. How to achieve stronger Consistency in terms of CAP Theorem. Consider [Spanner](./13.Spanner(Strong Consistency RW).md) and [Memcache](./16.Memcache(Cache Consistency).md)’s scheme Spanner Linearizability For R/W transaction, Use PAXOS and 2PC to write to remote replication. (Wait for quorum’s acknowledge) For R/O transaction, read from local. Use Snapshot Isolation and TrueTime clock mechanism to ensure local serializability. Memcache Eventual Consistency Introduce memcache layer to achieve write to DB and read from memcache. Write need receive DB’s acknowledge and read has no need to wait. For COPS, it figures out a new way to make more efficient read and write by implementing causal consistency, which is stronger than eventual consistency but weaker than linearizability. Implementation COPS, aka Cluster of Order-Preserving Servers, a key-value store that delivers causal-consistency model across the wide-area. Every Data Center has a local COPS cluster, which maintains a complete replica of stored data. So client can just talk to local cluster for data’s read and write. In each Data Center, data are divided into many shards, which is linearizable and clients access each partition independently. So the whole cluster is linearizable as well. The problem comes when clusters communicate with each other to remain sync. To achieve causal consistency, COPS have a prime node be responsible for local writing. After local writing is finished, prime will send it to other cluster’s prime node, and a version number will be sent as well to keep causal order. Causality ​ Potential Causality definition Execution Thread. If a and b are two operations in a single thread of execution, then a -&gt; b if operation a happens before operation b Gets From. If a is a put operation and b is a get operation that returns the value written by a, then a -&gt; b Transitivity. For operations a, b, and c, if a -&gt; b and b -&gt; c, then a -&gt; c Example: We can learn from the figure that put(z,5) is derived from get(x) = 4, which means to execute put(z,5) we have ensured that what is logically happened earlier than it. Note: If the system can not tell weather two operation’s happening order, since there is no explicit reason about they, We can simply define they are concurrent, so system can decide the order they happen. But for two put concurrently write to the same key, there is a conflict. So to deal with conflicts, Last write win is a proper way to deal with it but if we want to doappend &amp; atomical count like thing, this strategy may not work. Context Each client maintains a context to explicitly mark their state. After each operation, the client will append a entry of keys’ version. So DC can simply use this entries to verify the dependencies of one operation. Example: We have 3 Data Centers, and one client calls put(Z, value, Zver3, Xver2, Yver4) to put Z = value. To forward this operation to other DC, DC1 will check the dependencies of Zver3, Xver2, Yver4 in other DC, if others’ are not in this stage, it will wait till other DCs reach or exceed the dependencies’ version number Lamport Timestamp To achieve global order, COPS use Lamport Timestamp in higher bits + unique ID For Data Center in lower bits . Combining with Logical timeclock and Wall Clock, we can give a global sequence despite of large inaccuracy. 12Tmax = highest version seen (from self and others)T = max(Tmax + 1, wall-clock time) Write Client -&gt; Local Data Store Cluster -&gt; other DCs When client sends a put(key, value …) , client library will calculate the dependencies according to the context. The local prime will wait till cluster has indeed store all the dependencies( check by version number). Then send to remote clusters, do the same. Read Read from local cluster, the library function provide both read the latest version of key and a specific older one by explicitly send a context with get. limitation Causal Consistency can not be aware of external dependency. For example, Alice told Bob to check a new status of the key, and then Bob sent a get request via client. Now Bob may see old value of key because the system do not know that Alice calls Bob yields Bob’s get request. And this is also discussed by lamport in &lt;Time, Clocks, and the Ordering of Events in a Distributed System&gt; It’s hard to manage conflict, Last write win is not generic","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"COPS","slug":"COPS","permalink":"https://zjuytw.github.io/tags/COPS/"}]},{"title":"6.824 Memcache lecture note","slug":"6.824-16.Memcache(Cache Consistency)","date":"2021-08-09T09:13:23.342Z","updated":"2021-08-09T10:05:56.458Z","comments":true,"path":"2021/08/09/6.824-16.Memcache(Cache Consistency)/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-16.Memcache(Cache%20Consistency)/","excerpt":"","text":"Memcahce at Facebook Intro In a popular webserver scenario, we have web application that clients send request (especially most reads and a few writes) to Data Base, and as we both know things get worse when one peer in the system suffer a throughput bottleneck. To achieve better performance and also get stronger consistency. Single Web Server(eg, running on Apache Tomcat) + Single DataBase(eg, MySQL/ Oracle) ​ &#8595; Muti-Stateless Web Server + Single DB &#8595; Mutl-Stateless Web Server + DB cluster(sharded by key, in both scheme and table layer) ​ &#8595; Mutl-Stateless Web Server + Memcache (For speeding up reads) + DB cluster Implementation Facebook noticed that their customers consume an ordered of magnitude more content that they create, so fetching data is the domain element for the performance bottleneck. Also, they have various storage services, like MySQL, HDFS etc, which means a flexible caching strategy is needed. Finally, they came up with an architecture that separate caching layer from the persistence layer, which means that for a group a Web server, they combine with a group of Memcache to form a Front-End Cluster, then a Front-End Cluster combine with a data-completed DB to form a region(AKA Data Center). So as the distributed spread of region, users from different area of USA can access to the Web server with lowest latency by choosing different region. Because of the tolerance of stale message differs in different situation User can stand for transient stale data, but not too long User tend to observe their latest data after writing it So the Memcache can achieve eventual consistency by using its R/W strategy. 123456789Read Scheme: v = get(k) if v == nil v = fetch from DB set(k,v)Write Scheme: send k,v to DB delete(k) in MC Hint This scheme can not prevent users from seeing stale data If user read exactly after line 8, at this point, Memcache still holds the stale data but DB has updated the key to the new value Q: Why not delete key in the MC first before send k,v to DB? A: Because if at the time deleted the key in MC but another server did not see key in MC, it will send fetch to DB then may get the stale data that might be deleted afterwards and store to MC. Then MC may store the stale data until another write is fired up. Q: Why not just set(k,v) in MV in line 9 A : Because delete is idempotent while set is not. Check in the example: 12345C1 : x = 1 -&gt; DB C2 : x = 2 -&gt; DB set(x,2)C1 : set(x,1) // makes stale data stored Prime &amp; Secondary Scheme For many regions, there is one master region and many salve region Local Read and Prime Write For read, each FE read use Read Scheme in local region. This is super fast For write, slave’s write need to be send to primary region Prime&amp;Secondary replication, primary DB always send info to remote DB to stay in sync Performance Let’s talk about two parallel process strategies. Partition increase RAM efficiency that each Key just store once Not good for some hot keys Client may talk to many part for one website’s resource Replication Good for hot key Fewer TCP connection RAM wasted for more replica For Facebook’s architecture, we have two completed replicated asynchronized region that brings fault-tolerance also low-latency for different area’s user. In each region, FB partitioned DB then using many Memcache to cache hot keys to reduce DB’s load. There is also a regional Memcache cluster in each region to cache those not too hot keys. Lease FB uses lease mechanism to fix the Thunder Herd and Race Condition. Thunder Herd – If many FE are simultaneously read the same key from Memcache and at this time, one FE do a write() and delete the old key in Memcache. Then DB may have the risk of flooded by too many queries for one key. Race Condition – Example 123456C1 : get(k) -&gt; missC1 : read k from DB -&gt; value1 C2 : write k = value2 -&gt; DB C2 : delete(k) to MCC1 : set(k,v1) to MC// In this situation, stale data of value1 will store on MC forever Solution To each get(k), Memcache server should issue FE a lease for a period of time. Thunder Herd, if one FE get the lease, then others that also send get(k) will block till the first FE calls put(k,v, l) or lease expired Race Condition, C1’s get(k) will be issued a lease, but C2’s delete will invalid the old lease, the when C1 fetch value1 from DB then calls put(k,v1, l), the Memcache server will reject it. Extend Another introduce of twitter’s cache system in Twitter 内存缓存系统分析论文阅读","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"Memcache@FB","slug":"Memcache-FB","permalink":"https://zjuytw.github.io/tags/Memcache-FB/"}]},{"title":"6.824 Spark lecture note","slug":"6.824-15.Spark(Big Data Process)","date":"2021-08-09T09:13:23.335Z","updated":"2021-08-09T10:06:04.918Z","comments":true,"path":"2021/08/09/6.824-15.Spark(Big Data Process)/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-15.Spark(Big%20Data%20Process)/","excerpt":"","text":"Spark Introduce Spark is a successor of MapReduce to process distributed big-data computing tasks. Frameworks like MapReduce, Dryad, and Spark help data scientists to focus on their business rather than wasting time on designing the distributed tasks and fault-tolerance. There are some constraints in previous frameworks that MapReduce lacks abstractions for leveraging distributed memory so makes it inefficient for those that reuse intermediate results across multiple computations and lacks high interactive flexibility, programmers may have trouble implementing some complex algorithms. Spark is an implementation of a new abstraction called resilient distributed datasets(RDDs), which are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators. RDD RDD(Resilient Distributed Dataset) is a collection of Read-Only and Partitioned records. RDDs can only be created through deterministic operations on either 1) data in stable storage or 2) other RDDs. Spark uses Lineage to keep track of how each RDD is transformed from previous datasets. Spark provides Action as well as Transformation. Action calculate RDDs and gets a result. Transformation imports data from external sources or transform an old RDD to a new Read-Only RDD. Computation Schedule RDDs are stored in distributed servers, so when we need to do Transformation, systems need to fetch previous RDD in the corresponding servers. There are two kinds of Transformations that forms different dependency between RDDs Narrow Dependency : Each partition of the parent RDD is used by at most one partition of the child RDD. Wide Dependency : Multiple child partitions may depend on the parent RDD. Spark speeds up Transformation by optimizing the Transformations related to Narrow Dependency. First, narrow dependencies allow for pipelined execution on one cluster node, which can compute all the parent partitions. Second, recovery after a node failure is more efficient with a narrow dependency, as only the lost parent partitions need to be recomputed. In contrast, in a lineage graph with wide dependencies, a single failed node might cause the loss of some partition form all the ancestors of an RDD, requiring a complete re-execution. Overall, a RDD are consistent of the following elements: Its partitions Its parent partitions Transformation Its metadata(eg, data type, storage position etc.) When user calls Action to process computation on RDD, Spark will build different stages according to lineages. Hence, Spark can build a job stage that contains as many Narrow Dependencies as possible to speed up the whole system’s efficiency. The boundaries of the stages are the shuffle operations required for wide dependencies, or any already computed partitions that can short-circuit the computations of a parent RDD. After building the job stages, Spark then launches tasks to compute missing partitions from each stage until it has computed the target RDD. While scheduling Tasks, Spark assigns tasks to machines based on data locality. The task will directly be processed by those nodes that is already holds the partition needed in memory. Otherwise, if a task processes a partition for which the containing RDD provides preferred locations(eg, an HDFS file), we send it to those. Fault-Tolerance Spark can re-compute the content of a failed RDD by dependencies from lineage graph. But there is a wide dependency during the re-computation, which means we have to re-compute all the RDD it depends, also, Spark won’t store all the RDD in the memory, or it will soon run out of memory. So we have to manually do persist, if necessary. 1rdd.persist(REPLICATE) Conclusion Spark RDD has the feature of: Store all info directly on memory Interactive API Find both Narrow and Wide Dependencies, while narrow one is more efficiency Have Checkpoint to failover from wide dependency failure But we still need to be aware that Spark is not a replacement for MapReduce: For those model and algorithms already fit in MapReduce, Spark won’t have a more efficient result for them. &lt;h2","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"Spark","slug":"Spark","permalink":"https://zjuytw.github.io/tags/Spark/"}]},{"title":"6.824 CRAQ lecture note(Pending for updated)","slug":"6.824-14.OCC(FaRM)","date":"2021-08-09T09:13:23.329Z","updated":"2021-08-09T10:44:28.217Z","comments":true,"path":"2021/08/09/6.824-14.OCC(FaRM)/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-14.OCC(FaRM)/","excerpt":"","text":"FaRM (To be updated...) Introduce Microsoft’s main memory distributed computing platform, FaRM can provide distributed transactions with serializability, high performance, durability, and high availability using RDMA and a new, inexpensive approach to providing non-volatile DRAM. NVRAM Strategy to become non-volatile: Using battery as back-up power, once the electric power fails, the system goes with battery and do all context saving work then shut down Note: This scheme is just helpful when encounters power failure, is not applicable for hardware/ software failure. –Because otherwise the system just shut down directly. Optimistic Concurrency Control For a optimistic lock, we have 12345Xaction begin read all values without lock use a buffer to store write commit writeXaction end Before commit transaction to storage, system need to verify the validation of the transaction. If success, then commit, else abort all the operation related to transaction. 12345tx.create()o = tx.copyRead(OID)o.value += 1tx.write(OID, o)ok := tx.commt() Transaction Management Refer to the above figures about server layout and OCC commit protocol. Let’s talk about FaRM’s transaction management. FaRM uses OCC and 2PC to achieve its serializability. 2PC strategy Read without lock, read(&amp; value, &amp;version) use one-side RDMA to read Lock the revised data primary polling for data which is (use DRMA to poll) locked, so send reject VERS changed, then send reject else, then set the lock and send yes.( To avoid racing, use CHECK&amp;SET atomic operation here) do validation for those no changed shard To check if version changed or locked] Commit","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"FaRM","slug":"FaRM","permalink":"https://zjuytw.github.io/tags/FaRM/"}]},{"title":"6.824 Spanner lecture note","slug":"6.824-13.Spanner(Strong Consistency RW)","date":"2021-08-09T09:13:19.948Z","updated":"2021-08-09T10:05:42.935Z","comments":true,"path":"2021/08/09/6.824-13.Spanner(Strong Consistency RW)/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-13.Spanner(Strong%20Consistency%20RW)/","excerpt":"","text":"Spanner Introduce Spanner is Google’s scalable, multi-version, globally-distributed, and synchronously-replicated data. Spanner is the first global distributed Database and support external consistency. It is a big framework, but I just want to focus on some particular points – Mainly about external consistency and True-Time mechanism. You may look for more detailed illustration on Google’s Spanner paper or other’s blog about spanner. Consistency Assume that if we have 3 Data Centers, we have to send data across all data centers to keep consistency. To keep a strong consistency, Google uses paxos to send logs and reach consensus. Moreover, google has its True-Time mechanism to reach external consistency, but let’s talk about it later. R/W transactions For a transaction both need to read and write,spanner uses 2PC. Pick a unique transaction ID to identify Do all the reads first then do all writes Send read request to all leaders in DCs, then DC lock the resources and reply Choose a leader as transaction coordinator Send writes request to all leaders, and leader gonna send prepared msg to followers into paxos log to make sure leader isn’t crashed and lost lock Once one leader finishes promising to followers, it sends a Yes to the coordinator Once coordinate received all Yes signal, it start to commit writes to its followers Then tell other leader to commit R/O transactions For Read-Only transactions, spanner speeds up this type by no 2PC and no locks. Start with a question that why we not directly read the latest value of each key needed? Answer: For a transaction T3 that print(x,y), if we have the timeline of below: 123T1 : wx,wy,commitT2 : wx,wy,commit T3 : Rx Ry T3 just saw x,y yielded by different transaction which breaks the serializability. From the example above, we know that our read need to fetch data in the same version. So spanner need to at least reach level of Snapshot Isolation. &lt;h4&gt; Snapshot Isolation&lt;/h4&gt; Spanner gives each transaction a timestamp, which makes all transactions execute in Timestamp order. 12R/W&#x27;s TS = commit timeR/O&#x27;s TS = start time Because Spanner has a multi-versions DB, that stores many versions (Not all version but a transient period’s versions). For R/O Xactions, DB can find the value with latest version less than R/O’s start time. Example: 123456T1 @TS10: wx,wy,commit() ⬆x@10 = 9, y@10 = 10T2 @TS20: wx,wy,commit ⬆x@20 = 8, y@10 = 12T3 @TS15: Rx Ry ⬆ read the TS of 10 Q: what if local replica is minority, how to get latest version less than TS? A: Every paxos peer gets log from leader, if one follower’s last log’s Timestamp &lt; TS, it will wait for leader’s msg till last log’s TS exceeds required TS True-Time mechanism Because of working volts and inner CPU frequency , it is likely every DC’s time can not keep sync without outside’s help. We have two consequence: R/O transaction’s TS too large Answer: correct in practice but slow, it will wait for paxo replicas to catch up R/O transaction’s TS too small Answer: it may miss recent writes, and not external consistent In Spanner’s scheme, Google has a satellites to keep synchronize its official time to each DC. In considering of latency in transportation, Spanner give each TT a range to mark its {earliest, latest} arrival time. 1TT interval = [Earliest, Latest] And we have a start rule: TS = TT.now(). latest For R/O, TS is the latest TT on start For R/W, TS is the latest TT on commit Commit Delay strategy R/W transaction delays till transaction’s commit time &lt; TS.now().earliest To make sure commit time in the past. Extends There are many details I haven’t covered, if you are interest in them. You can just search on Google for English Blogs, as for Chinese Blogs, I do recommend Google-Spanner 论文的思考 and Spanner, 真时和CAP理论 ​","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"Spanner","slug":"Spanner","permalink":"https://zjuytw.github.io/tags/Spanner/"}]},{"title":"6.824 Frangipani lecture note","slug":"6.824-11.Frangipani(Cache Consitency)","date":"2021-08-09T09:12:42.351Z","updated":"2021-08-09T10:05:31.237Z","comments":true,"path":"2021/08/09/6.824-11.Frangipani(Cache Consitency)/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-11.Frangipani(Cache%20Consitency)/","excerpt":"","text":"Frangipani(To be updaed) Introduce While Frangipani is a paper published in 1997, I just want to talk about cache consistency in detail. The paper are mainly discussed following 3 aspects: cache coherence distributed transactions distributed crash recovery Cache consistency Rules for cache coherence No cached data without data’s lock acquire lock then read from petal write to petal then release lock Extends For more details, you may refer to this BlogFrangipani: A Scalable Distributed File System 论文阅读","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"Frangipani","slug":"Frangipani","permalink":"https://zjuytw.github.io/tags/Frangipani/"}]},{"title":"6.824 Aurora lecture note","slug":"6.824-10.Aurora","date":"2021-08-09T09:11:36.427Z","updated":"2021-08-09T10:05:22.628Z","comments":true,"path":"2021/08/09/6.824-10.Aurora/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-10.Aurora/","excerpt":"","text":"Aurora Introduce Amazon Aurora is a relational database service for OLTP workloads offered as part of Amazon Web Services. Aurora is designed to address the constraint of bottleneck of network throughput, it also allows for fast crash recovery, failovers to replicas and fault-tolerant. History EC2 EC2 is Elastic Cloud 2 for short. Users can rent instances of EC to deploy their Web Server or DB services. Each instance of EC2 are running in the Virtual Machine on the physical node, and all their storage is redirected to a external locally attached disk via VMM(Virtual Machine Monitor). For stateless web server, EC2 is convenient for its scalability and high-performance For a storage system like DB service, there are bunch of contraint: Limited expansion : MySQL on EC2 is not able to do write expansion Limited fault-tolerance : Once the node fails, we can not access to locally attached disk for data EBS EBS is Elastic Block Store for short. It is the progress of EC2 that Amazon uses a multiple instances of EBS to do a Chain-Replication to have fault-tolerance. Constraints: Network bottleneck because of large amount of data is sending by network Not FT, for Amazon always put EBS in same Data Center. RDS To deal with the constraints mentioned above, Amazon provides a more fault-tolerance system, Relational Database Service Compared with EBS, RDS can survive if a whole AZ(Available Zone) fails, but have to send write between primary and replica, which means the performance of write decreases as well as the data of cross-AZ increases dramatically. Aurora For a new system, Amazon was eager to have both fault-tolerance and performance done well, as following: Write although one AZ down Read although one AZ down + one replica down Minor slow won’t affect overall efficiency Fast Re-replication Feature of Aurora: Only send log record– The storage server can apply the log to page, so Aurora can just apply log without applying dirty page, which reduces the network workload Only 4 Nodes required to make consensus Quorum Scheme If we have: N Replicas W for Writers’ consensus to move R for Readers’ consensus to move R + W = N +1, this makes sure W &amp; R will get least one overlap Example: 123N = 3R = W = 2 or R=3, W = 1We can adjust speed of R or W by adjusting the number of them In Aurora, N = 6, W = 4, R =3 Conclusion In a word, Aurora optimized data transportation type and used quorum write scheme which got 35 times speeds up compared with RDS’s MySQL. Extends You can find more detailed description of Aurora’s work flow in Amazon Aurora: 避免分布式一致性 and 浅谈Amazon Aurora","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Aurora","slug":"Aurora","permalink":"https://zjuytw.github.io/tags/Aurora/"},{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"}]},{"title":"6.824 CRAQ lecture note","slug":"6.824-9.CRAQ","date":"2021-08-09T09:08:08.068Z","updated":"2021-08-09T10:05:27.977Z","comments":true,"path":"2021/08/09/6.824-9.CRAQ/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-9.CRAQ/","excerpt":"","text":"CRAQ Introduce Start with awareness that Paxos, Raft’s consensus algorithms have bottleneck in leader, because leader need to send logs to all followers and wait for majority of their reply. Chain replication is purposed in 2004, its scheme ensures serializability meanwhile the whole workload is distributed among the system(to each node). CRAQ is an improvement on Chain Replication, maintains strong consistency while greatly improving read throughput. But in this article, I will just mainly talk about Chain Replication. Chain Replication Use the chain on Figure 1 and a remote coordinating cluster like ZK, Raft or Paxos to check heartbeat and send configurations to nodes on the chain. Failure Recovery Head fails: Its successor becomes head Tail fails: Its predecessor becomes tail Intermediate fails: predecessor send MSG to its successor Evaluation Pros: Head’s workload is far less than Raft’s leader’s. Because leader needs to send sync. packets and handle R/W log Cons: Every node can slow down the whole system. Raft, instead, just need a majority of nodes keep running. Extend To read more about CRAQ, you may find articles in 浅谈Primary-Back Replication和Chain Replication","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"CRAQ","slug":"CRAQ","permalink":"https://zjuytw.github.io/tags/CRAQ/"}]},{"title":"6.824 ZooKeeper lecture note","slug":"6.824-8.ZooKeeper","date":"2021-08-09T09:07:17.945Z","updated":"2021-08-09T10:44:13.292Z","comments":true,"path":"2021/08/09/6.824-8.ZooKeeper/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-8.ZooKeeper/","excerpt":"","text":"ZooKeeper(To be continuely updated) Introduction ZooKeeper is a very popular service for coordinating processes of distributed applications, it provides a simple and high performance kernel for building more complex coordination primitives. We users can deploy ZK in many distributed applications like, services registration and discovery, cluster management as a coordinator. ZooKeeper has the following features: Sequential Consistence All client see the same data that one client’s transactions will applied on ZK in its original order Atomicity Single View Clients see the same data no matter it talk to which server High performance High availability Implementation The node in the tree is called znode, which stores data and node information. The main task for ZK is to coordinate but not file storage, so znode’s file size is smaller than 1MB There are two types of znode Ephemeral : ZK will automated delete it, after session finishes Persistent : Need client to delete explicitly Node information Znode has a sequential flag, it will be issued a monotonically increased number if flag is true when created, to mark the global sequential order of the znode. It also maintains a state information table call Stat. Sequential Consistency To achieve sequential consistency, ZK uses its own ZAB consensus algorithm, like Raft and Paxos in implementation but different in some details. ZK guarantees the single client FIFO transactions order. For R/W, ZK has different rules For reads, Leader/Follower/Observer all can directly handle read request. (read locally) For write, all writes requests need to send to leader then wait till reaching consensus. Note: For those need read a fresh data, client may send a to leader, then send read to replica. Conclusion This is a simply discussion about ZK, I am just dabble in distributed systems, so I will keep updating this article as my concept of ZK grows","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://zjuytw.github.io/tags/ZooKeeper/"}]},{"title":"6.824 Lab4","slug":"6.824-lab4","date":"2021-08-09T08:55:23.224Z","updated":"2021-08-09T10:47:25.908Z","comments":true,"path":"2021/08/09/6.824-lab4/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-lab4/","excerpt":"","text":"Lab4 Overview Finally, we make it to the final lab, no doubt that it’s the most challenging work among 4 labs. In this part, we gonna implement two main K-V storage system, one is shardmaster and one is shardkv. The specific service of them will be discussed in FRAMEWORK part, let’s talk how much workload this lab may take first. Compared with former labs, we have no paper to refer to and no framework to follow. This lab emphasizes more on real producing environment that we need balance load (so shard comes) and make shard movable among muti-raft. So shardmaster is a configuration center that decides which raft group servers which shards, and shardkv is exactly one raft group that need to provide PUT/APPEND/GET service to their served shards. (I will put a figure in **conclusion **to make you easier to understanding) Lab4A Now we are to build shardmaster, which is a group consensus configuration center and records current serving raft groups and which shards one raft group servers. Our task is to adjust shards and dispatch every configuration it receives. The whole design frame is alike as lab3, so we can just copy lab3’s code and revise some part to satisfy the need of Rebalance Shards in each raft group. So what we gonna do is: Build the basic K/V storage system based on lab3 Design a Shard Rebalance mechanism that meet the lab’s request that The shardmaster should react by creating a new configuration that includes the new replica groups. The new configuration should divide the shards as evenly as possible among the full set of groups, and should move as few shards as possible to achieve that goal. The shardmaster should allow re-use of a GID if it’s not part of the current configuration Implement Join( ), Leave(), Move(), Query() Implementation The whole design is similar to kvraft, because we have implemented consistency and deduplicated underlayer, we can just mainly focus on Join() &amp; Leave( ). I just simply use the strategy that move one shard from largest group (which owns most shards) to smallest group. The boundary condition is: Group 0 should have no shards, which means once group 0 has shard, we should select it as largest group and no select it when its shards is least. Every raft group’s shards disparity should &lt;= 1 So let’s consider query() and move() first: For query(idx int), just return idx-th index’s config and return latest config if idx == -1. Note : If idx exceeds the array length, we should return a invalid config for upper layer’s double check rather than throwing a exception and exit. For move(), we just move one shard from one raft group to another. No more adjusting needed. Then Join() and Leave(), these two function have both Rebalance Shards part which need we think carefully before coding. There is two allocation strategies: one is that I formerly described and another is consistent hash, it is a more realistic way to address load balance. (But I just use the straightforward way). Once we decided the rebalance strategy, what left is much easier that to handle Join(), we can just append a group has no shard then call rebalance(), to handle Leave(), we can just assign left group’s shards to group 0 then call rebalance(). Once one of join, leave, move is done, append a new config corresponding to it to configs. Lab4B This part takes me longest time to consider whole logic and the same time to debug. So I hope there are some problem to be addressed before programming. First, what should one shardKV do. shardKV is a raft group which shardMaster previously recorded in config, to make you easier to understand, the normal format is like: “[server-101-0 server-101-1 server-101-2]” which means raft group 101 has 3 raft peers from 0 - 2. So these 3 servers is required to provide Key/Value service for their served shards. So simply speaking, if shards are statistic, a shardKV is just like kvraft. Second, shards movement. Because shards are not fixed on one group, we should move shard from one group to another, so problem comes that which information should be delivered and how to make all group remains consistence. Third, which operations should be asynchronized in a group. If you haven’t have a figure of the whole frame in your head it’s OK, but let’s image first what we should do in this program. Start with configuration fetch, we should routinely check if there is a newer config then try to update. Then the migration method, it is unwise that once you need to get or send one shard’s data, you just stuck and wait for a reply. Third the GC thread, (optional) Finally, how to achieve consistency on moving shards. It’s not hard to answer, we should just use leader to launch every request, use rf.Start() to record, once received command in AppCh, then start to process the command. Implementation To pass all test, we should consider Garbage Collection and Non-Stuck service when other shards are pulling. In whole program, we mainly implement ConfigFetch(), ShardMigration(), ShardGC(). All data structure and data stored is involved with these 3 main functions. Config Update As we discussed above, we will launch a thread to routinely fetch new config. Once we get a new config whose version number is exactly current number + 1, then we should set current config as new one. But here is the problem, how to deal with old data? No matter we are gonna fetch shard from shrink group or push shard to escalated group, we both need to know the RPC address by make_end(server_name string). So we need at least one previous config to send RPC call, **but do we need more previous config? ** If we apply every config once we detect a new one, it is likely we haven’t pull or push data before config change. So we need to save all previous config, and make sure one version’s shard data is all transported then we can recollect the memory. But it is much tremendous compared with another implementation I learned from Xinyu Tan’s Repothat, we just wait till all group’s all shards are ready. So we can make sure that all group which apply new config are clear with previous shards’ data. ShardMigration For one shard move from one group to another, it is both OK that we can choose pull or push from either side. In my implementation, I choose to pull from shrink group. Now we can consider what data should be stored, like previous lab, we need both kvtable &amp; ClerkRecord to deduplicate. It is a general thought we can store these info. for every shard and migrate with shard. Also, we need a status id to notify current status. We need 4 status to mark different states. Serving (ready state) Pulling (Config changes and this shard is prepared to serve after pulling data from previous one) BePulling(Config changes and this shard is prepared to be pulled data by new group) GCing (After fetch shard’s data, we need a new RPC call to tell previous one to delete shard) ShardGC For those shards shrunk from previous config to current one, we are respond to recollect these memory. So we design a new mechanism to process GC, which means once new group successfully fetched shard data from previous one, (Pulling -&gt; GCing), there are a thread routinely check GCing state and send GC signal to other group to tell it, it’s OK to delete data. Hints Every shard can receive request from client if they are in Serving and GCing All Thread should check if it’s leader, then begin to process formal job To deal with unreliable network, we should use configure number to check if is the right request. In GC step, the GCed group should first use Raft record the GC command then delete and send reply.(Or once this time it crashed, no one will notify it to GC again, because the current shard’s group has received the reply and turn GCing to Serving) Time to detect Wrong Group In previous design, we just exam Sequence number, but now we should do group exam to block those not serving shard’s request It is not enough to exam it at first, just imagine a situation 1234567891011121314func (kv *ShardKV) putAndAppend(cmd *Op)&#123; key, value, op, cid, seq := cmd.Key, cmd.Value, cmd.Op, cmd.Cid, cmd.Seq kv.mu.Lock() defer kv.mu.Unlock() //shard := key2shard(key) //if ok := kv.checkShard(shard); !ok&#123; // cmd.Err = ErrWrongGroup // return //&#125; ok := kv.updateClerkMap(cid, seq, shard) if ok&#123; then ... &#125;&#125; ​ if we do not exam shard in Put, Append, Get, we may have inconsistency consequence Shard have updated(Because of asynchrony flow), and it is now not serving in this group. Data do have Put, Append, Get and clerkRecord updated. But it won’t be watched in new serving group So we need a mechanism to tell previous Public interface (Put(), Append(), Get()) that our shards have changed, that there is an ErrWrongGroup out there. Thus I adapted a different way to watch progress of each operation, channel. That every operation gets its channel identified by the return index value of rf.Start(). So once the command is executed, the kvraft will notify interface that operation has been done by channel(as well as err). 123456789101112131415161718192021222324252627func (kv *ShardKV) getChan(raftIndex int, create bool)chan Op&#123; kv.mu.Lock() defer kv.mu.Unlock() if _,ok := kv.chanMap[raftIndex]; !ok&#123; if !create&#123; return nil &#125; kv.chanMap[raftIndex] = make(chan Op,1) return kv.chanMap[raftIndex] &#125; return kv.chanMap[raftIndex]&#125;func (kv *ShardKV) listen(ch chan Op, index int) Op&#123; select &#123; case op,ok := &lt;-ch: if ok&#123; close(ch) &#125; kv.mu.Lock() delete(kv.chanMap, index) kv.mu.Unlock() return op case &lt;- time.After(time.Duration(500) * time.Millisecond): return Op&#123;Err: ErrWrongLeader&#125; &#125;&#125; Test Result: 100 times tests result:","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Lab Note","slug":"Lab-Note","permalink":"https://zjuytw.github.io/tags/Lab-Note/"},{"name":"ShardKV Storage","slug":"ShardKV-Storage","permalink":"https://zjuytw.github.io/tags/ShardKV-Storage/"}]},{"title":"6.824 Lab4 Conclusion","slug":"6.824-lab4-Conclusion","date":"2021-08-09T08:55:23.223Z","updated":"2021-08-09T10:06:50.016Z","comments":true,"path":"2021/08/09/6.824-lab4-Conclusion/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-lab4-Conclusion/","excerpt":"","text":"Conclusion In this part, I will illustrate more detailed implementation of ShardKV based on the expectation that you have read the lab4 and know the basic idea of the framework. And I will describe following the sequence of a message sent by Clerk and to each part of Raft Group. Implementation ShardKV As we described previously, we should maintain a Shard Map, config &amp; lastConfig information in each Raft Group to complete 3 main functions. 12345678910111213type ShardKV struct&#123; ... chanMap map[int] chan Op lastcfg shardmaster.Config cfg shardmaster.Config servedShard map[int]Shard&#125;type Shard struct&#123; Table map[string]string ClerkRecord map[int64]int Status uint8&#125; Interface(Put, Append, Get) To receive ErrWrongGroup msg returned while true executing, we need to listen to each channel. We can allocate channel according to the Raft’s log index. What’s more, because of raft’s reelection, the map of index -&gt; channel is not unique, so we should also check whether the value channel returned is what you want. K/V table & ClerkRecord Just change the manipulating data from global map to each shard’s own map Config Update One of the threads watching for new configurations, and once finds a new confg, send a command to Raft to make a consensus 12345678910func (kv *ShardKV) pullConfig()&#123; if !IsLeader() || !IsAllReady()&#123; return &#125; nextcfg := kv.cfg.Num + 1 cfg := kv.mck.Query(nextcfg) if nextcfg == cfg.Num&#123; kv.rf.Start(cfg) &#125;&#125; Once applied a new configuration, we should figure out MoveIn &amp;&amp; MoveOut. For those MoveIn, turn status to Pulling, waiting for pulling data from previous group. As for MoveOut, turn status to BePulling, waiting for other’s pulling and GC. Note: Configuration update should be done after under layer rafts reach a agreement. Shard Update We should launch a thread to routinely pull shards whose status are “Pulling”. So we should design a RPC call arg and reply, We need Confignum to mark the sender’s version in args and Shard Num and ConfigNum in reply because we just simply put the reply into Raft to record command of Update Shard later. 123456789101112type PullShardArgs struct&#123; Shard int ConfigNum int&#125;type PullShardReply struct &#123; Err Err Shard int ConfigNum int Table map[string]string ClerkRecord map[int64]int&#125; 12345678910111213141516171819202122232425262728293031323334func (kv *ShardKV) pullShard()&#123; if !IsLeader()&#123; return &#125; moveIn := kv.getShardsByStatus(Pulling) var wait sync.WaitGroup for _, shard := range moveIn&#123; wait.Add(1) go func(shard int,cfg shardmaster.Config) &#123; defer wait.Done() gid := cfg.Shards[shard] servers := make([]string, len(cfg.Groups[gid])) currentCfgNum := cfg.Num + 1 copy(servers, cfg.Groups[gid]) for i:= 0; i &lt; len(servers); i++&#123; server := servers[i] go func() &#123; srv := kv.make_end(server) args := PullShardArgs&#123; Shard: shard, ConfigNum: currentCfgNum, &#125; var reply PullShardReply RPC call(&amp;args, &amp;reply) if successful&#123; kv.rf.Start(reply) &#125; &#125;() &#125; &#125;(shard, kv.lastcfg) &#125; kv.mu.Unlock() wait.Wait()&#125; After receiving PullShardReply in appCh, we are gonna try to update shard. 12345678910111213if cmd.ConfigNum == kv.cfg.Num&#123; table := make(map[string]string) cRecord := make(map[int64]int) deepCopyTable(table, cmd.Table) deepCopyRecord(cRecord, cmd.ClerkRecord) if kv.servedShard[cmd.Shard].Status == Pulling&#123; kv.servedShard[cmd.Shard] = Shard&#123; Table: table, ClerkRecord: cRecord, Status: GCing, &#125; &#125;&#125; Garbage Collection Once new shard receives its data from old one, it is responsible to send GC signal to old group to help it delete useless shard data. So we need a new RPC call type and a thread routinely check if there is shard status is GCing, if yes then send GC RPC shard by shard. Note: one trap here is, because of unreliable network, there may be a situation that after received GC signal old one has deleted the data but new shard doesn’t receive the reply. So old group move on, but new one remains sending GC but no reply. Solution is, let GC() handler return OK whenever it receives an outdated ConfigNum request. To delete shard, the old group should wait till Raft reaches a agreement then reply a OK message. Or due to lose of command in Raft, there may just one peer( the old leader ) deleted the shard Snapshot Just keep your new data persist Some key points Pay attention to reference copy, especially the map sending from Raft’s channel A big trap here is, the whole system may stuck in a livelock due to restart. The situation is, because raft leader need a current Term’s log to update commit Index. But once there is a restart of raft, plus there happen no new command comes in. The raft won’t commit nothing, and because this group haven’t replayed to newest state, other group can not move on either( They count on the stuck raft to make GC or Pull date from the raft). And the solution is commit a blank command whenever a new leader is selected to help raft move on Believe you can finish this!!!","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Lab Note","slug":"Lab-Note","permalink":"https://zjuytw.github.io/tags/Lab-Note/"},{"name":"ShardKV Storage","slug":"ShardKV-Storage","permalink":"https://zjuytw.github.io/tags/ShardKV-Storage/"}]},{"title":"6.824 Conclusion","slug":"6.824-lab3-Conclusion","date":"2021-08-09T08:53:11.992Z","updated":"2021-08-09T10:06:39.243Z","comments":true,"path":"2021/08/09/6.824-lab3-Conclusion/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-lab3-Conclusion/","excerpt":"","text":"Conclusion Lab3 is a tough journey, especially when you finish the whole framework, then problem just starts. I took roughly two days to complete my draft and spent 2-3 times time to debug. In order to remind me of this hard time, spending hours reading 20000 lines log, suspecting each line I wrote and make you who are reading this avoid bugs or debug smoothly, I will write problems as explicit as I can. To make my design easy to understand, I will start with design graph. Structural Design Specific Implementation Overview Because detailed Raft design is described in former part, so let’s just assume we all pass Lab2 perfectly (Check it every time you modify Raft’s code!) For KV Server In 3A, we achieve the functions to send command and listen to channel till Raft peers reach agreement and persist the command, once Server receives specific message, it should execute the command(Without repetition). In 3B, we will implement snapshot. This idea is straightforward but we need to overcome the side-effect it brings. For Raft We need not modify our Raft design in 3A In 3B, we should modify our code to deal with what Snapshot brings the first problem is to make your Raft execute more fast to pass TestSnapshotSize3B. What time to tell KVServer to snapshot Index Convert Concrete Plan KVServer To avoid repeating execution, assign each clerk an unique ID and mark each command a monotonically increasing sequence number. So Server can exam this info. which stored in map to decide whether execute. KVServer:: Get() &amp; PutAppend() A tricky implementation here is, in Get() we can achieve faster response by check the ClerkID’s current sequence number to get value directly. You can do timeout check by Receive a event of timer.After or Condition variable of check a count value KVServer:: commitCommandHandler() A thread function to listen to appMsg Channel. If receives message, it will do Valid check, Index check and Command check to execute a command in right way. Note: Because the channel is a FIFO sequence, Server‘s execution sequence is decided by Raft’s push sequence. It’s a nice point that we don’t need to pay extra attention keep Raft and KVServer in sync, especially when Raft requires for a Snapshot in a very index. appMsg Channel with buffer To make execution more efficient and decouple Raft and KVServer. We can preallocate a buffer for this channel so Raft‘s log can be pipelined down into channel. **Note:**Don’t worry about inconsistency, Server will still execute in sequence and Raft will still get the right index’s Server’s state Snapshot. Raft Remember to check your Raft first when you encounter a problem!!!(90% bug) I actually don’t wanna talk a lot about basic design here, for InstallSnpshot RPC is much similar as previous design. But there are still some point I want to record. To achieve high concurrency. In previous part, I just make HeartBeat &amp; EntriesAppend a same mechanism to send : goroutine check rf.log every 100ms and send new log if nextIndex[i] &lt;= len(log) - 1. Now we can add an extra send action in Start( ) Note: Be careful when you do so. Imagine a partition situation: a network-failed leader gonna send his log to others each time it receives new log. Once it’s reconnected and receives info. from current leader and is ready to become follower, but sendAppendEntries has been called. So a packet with up-to-dated term and false log will be received and accepted by other followers. The problem is caused by checkStatus() step and sendAppendEntries() are not atomic, and in low concurrency situation this failure somehow are less likely to happen. Time point of chitchat Server should send maxRaftState to Raft during init(), each time Raft do a commit, it should check RaftStateSize and send signal if oversize. Each time Raft truncates log, lastApplied should be changed as well as IastIncludedIndex Note: I read other’s Blog and they all mentioned one problem of deadlock which happens between Server and Raft. In my design, we don’t need to worry about it, for each layer is working just at its own range. If you design a structure that Locks kv.mu and rf.mu in same time, you should be aware of the bad consequence it may cause. A weird slice reference problem happened on me Problem description: Leader has a log : [A, B, C, D, E] for short, and in one moment, the AppendEntries log it send became : [C, D, C, D, E]. I checked locks and run hundreds of time of race detection with race-free. It costs nearly 1 day to locate and find reason. If you aren’t interested in this problem, just keep in mind: Golang’s slice has a implicit mechanism of copy and truncate, each time you use newdata = data[start : end], you have the idea that newdata will be effected as long as data changed. The bug will sometimes even make no sense when you do more operation on newdata. Now let me intro my bug: ​ Think about what Test will print It seems slice are not allocate b a new space, so b’s pointer is point to data’s physical array[2], what makes thing worse is 1234data[0] = data[4]tmp := data[5 : ]data = data[:1]data = append(data, tmp...) also not assign a new memory to data. ​ So this is the result But you can just use 1data = data[4:] to truncate your log, because it will allocate a new array to data. Now let’s check my code 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/* Called by KVServer to set Snapshot*/func (rf *Raft) SetSnapshot(snapshot []byte, index int)&#123; rf.mu.Lock() defer rf.mu.Unlock() if rf.lastIncludedIndex &gt;= index &#123; return &#125; //rf.log[0] = rf.log[rf.logIndexToArrayIndex(index)] //tmp := rf.log[rf.logIndexToArrayIndex(index+1):] //rf.log = rf.log[:1] //rf.log = append(rf.log, tmp...) rf.log = rf.log[rf.logIndexToArrayIndex(index) : ] rf.lastIncludedIndex = index /* Be careful, the lastApplied need to be updated as well */ rf.lastApplied = index ...&#125;func (rf *Raft) startAppendEntries() &#123; //c := time.Now() rf.mu.Lock() if rf.status != LEADER&#123; rf.mu.Unlock() return &#125; index := len(rf.log) - 1 + rf.lastIncludedIndex term := rf.currentTerm leaderid := rf.me leaderCommit := rf.commitIndex for i := 0; i &lt; len(rf.peers); i++ &#123; var logs []Pair var prevLogIndex, prevLogTerm int if i == leaderid &#123; continue &#125; if rf.nextIndex[i] &lt;= rf.lastIncludedIndex&#123; //gonna send snapshot go func(i int, lastIncludedIndex int, lastIncludedTerm int, data []byte)&#123; /* Install Snapshot RPC*/ ... &#125;(i, rf.lastIncludedIndex, rf.log[0].Term, rf.persister.ReadSnapshot()) continue &#125; if index &lt; rf.nextIndex[i] &#123; prevLogIndex = index prevLogTerm = rf.log[rf.logIndexToArrayIndex(prevLogIndex)].Term &#125; else if index &gt;= rf.nextIndex[i] &#123; logs = rf.log[rf.logIndexToArrayIndex(rf.nextIndex[i]) : rf.logIndexToArrayIndex(index+1)] prevLogIndex = rf.nextIndex[i] - 1 prevLogTerm = rf.log[rf.logIndexToArrayIndex(prevLogIndex)].Term &#125; go func(i int, prevTerm int, prevIndex int, logs []Pair) &#123; args := AppendEntriesArgs&#123; Term: term, LeaderId: leaderid, PrevLogIndex: prevIndex, PrevLogTerm: prevTerm, Entries: logs, LeaderCommit: leaderCommit, &#125; reply := AppendEntriesReply&#123;&#125; ok := rf.sendAppendEntries(i, &amp;args, &amp;reply) rf.mu.Lock() ... &#125; rf.mu.Unlock() &#125;(i, prevLogTerm, prevLogIndex, logs) &#125; rf.mu.Unlock()&#125; I just implicitly use the reference of log and pass it to the closure to call RPC. Although I lock the whole body of startAppendEntries(), the context within the closure is not atomic, so left log a time window to make change. When you encounter with other bugs: DPrint is a good helper to locate errors, and make sure every variable goes on track.","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"KV Raft","slug":"KV-Raft","permalink":"https://zjuytw.github.io/tags/KV-Raft/"},{"name":"Lab Note","slug":"Lab-Note","permalink":"https://zjuytw.github.io/tags/Lab-Note/"}]},{"title":"6.824 Lab3 KVRaft","slug":"6.824-Lab3","date":"2021-08-09T08:53:03.692Z","updated":"2021-08-09T10:06:36.028Z","comments":true,"path":"2021/08/09/6.824-Lab3/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-Lab3/","excerpt":"","text":"Lab3 Overview In this Lab, we are gonna build a Key/Value storage system based on raft, AKA KVRaft. In part A, just maintain a in memory Hashmap of Key -&gt; Value and In part B, we need to deal with the growing Hashmap and use Snapshot to discard old log entries. Some details may be discussed in each section. Lab3A To implement a KVRaft, we use clerk to denote a interface the client calls to use K/V storage, server to denote a peer among K/V storage system which is based on Raft protocol. Particularly, we need to address of some detailed problems Leader find Whether make server report to clerk which is leader(Not applicable for this lab) Repeat request Like, send append request to a raft while due to the unreliable RPC, before receiving a commit server receives a “timeout”. So when you are trying append secondly, you should be careful not to append twice. Be careful of deadlock Implementation To find leader, clerk just use randomized generate server_id to query for true leader, and once get positive response clerk will record this id for later use. Furthermore, I want to explain why server not send a leader_id to clerk. In this lab3a, the server list every clerk maintains are different as I Dprint(ck.leader) shows, which means there are not a standard leader_id for all clerk Generate a random id for each clerk and mark each operation a monotonic increasing sequence number to erase repeat. The erase repeating step should be processed in server level, not in raft level. For these out-dated operation, because the sequence number for each clerk is monotonically increasing, server just skip manipulating the data but instead return positive Lab3B This time, we are gonna add snapshot feature to make raft discard old log entries. It requires us to modify our original Raft design and implement some new functions and InstallSnapshot. In my structure, KV server will tell Raft maxRaftSize , so when log’s size is greater than this, Raft will notify server and require for a snaphot for current_Index (a tricky point is, because KV server receive msg from channel one after another, there won’t be concurrency problem). And once KV server send a snapshot to Raft, Raft just save it and discard old entries. When crash happens, Raft should send the newest snapshot to server. Implementation Interface design Raft :: SendMaxRaftState(), (To be called by Server) Raft :: applyMsg{} :: Command -&gt; “Require Snapshot” Raft :: SendSnapshot(), (To be called by Server to send snapshot to Raft) Raft :: applyMsg{} :: Command -&gt; “Commit Snapshot” Index convert To discard old log, we should keep last Snapshoted Index, I use lastIncludedIndex to denote it. When we do Snapshot or Install Snapshot, we all need reset lastIncludedIndex Do index convert modification in each code uses rf.log Usage of snapshot when Raft crashed and reboots, it should send KV server its snapshot and restart at lastIncluded index if a leader detects one follower’s nextindex falls too behind and its nextIndex is lower than truncated leader’s index, Leader should send it a snapshot. Every snapshot is accompanied with truncated index, if receiver’s lastIncludedIndex is larger than the truncated index, then just return Some detailed hints Raft need persist lastInculudedIndex just for restart use. When lastIncludedIndex changed, lastApplied need to be updated as well Chitchat point between KVServer and Raft. Raft will send appMsg to KVServer to keep in synch. overall To send snapshot, KVServer calls Raft’s public interface I will describe these in detail in Conclusion.md Test Results: 100 times batch test:","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"KV Raft","slug":"KV-Raft","permalink":"https://zjuytw.github.io/tags/KV-Raft/"},{"name":"Lab Note","slug":"Lab-Note","permalink":"https://zjuytw.github.io/tags/Lab-Note/"}]},{"title":"6.824 Lab2 Conclusion","slug":"6.824-lab2-Conclusion","date":"2021-08-09T08:41:55.508Z","updated":"2021-08-09T10:03:38.200Z","comments":true,"path":"2021/08/09/6.824-lab2-Conclusion/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-lab2-Conclusion/","excerpt":"","text":"Conclusion After finish 3 parts of Lab, I have implemented a basic raft protocol and it can used to build a K-V storage server. It is fascinating for its understandability and straight idea to build this consensus system. In past 3 parts, I achieved timeout election, append entries, persist and many subtle details to ensure the consistency. My implementation may seem somehow ungraceful especially when I looked the tutorial course 'Go thread and Raft', so I do recommend you to have a look at it before you start your first line of code. storage server 1 storage server 2 storage server 3 K/V service K/V service K/V service Raft Raft Raft In this distributed storage system, clients talk to application layer like K/V service, and application send info to Raft to make distribute replicas. Usually, Raft is a library to be included and easy used Features of Raft Fault Tolerance(using 2n + 1 servers) To avoid split brain, adapt majority voting strategy. The basic logic is if there is a split brain, can not have both partitions own majority server. In Raft’s detail, it means that for new leader, it will assemble at least one server’s vote which from previous majority. We should use one of those previous majority servers as new leader because we should ensure new leader must know what happened before(What term is before). TODO (ALL DONE) I still have some issues need to be addressed, some are performance related and some are many bugs(I don’t know for sure, for me, I’d rather blame these FAIL to too strict test time restriction…) Some time, many 1 out of 10 the test will fail, for the program can not reach agreement. And the reason is easy, timeout election can sometime spend much time to elect a leader. And followings are solutions, from my point. Check if two peer are racing like, one elected in Term x but in Term x + 1, another timeout and start a new round election. For this situation, consider if timer triggers before you reset it Sleep time is a mysterious number I’ve changed them for many times but still have some problem… Currently a fair sleep time is [200, 400] for heartbeat timeout and [50, 100] for election timeout Some threads’ synchronization is implemented heavily so need a more soft way to achieve it(As I will describe afterwards) Some terrible codes need refactoring like substitute all channel by using condition variable and merge analogous code into one function. Especially, primary should become leader as long as it receives majority votes, not until all RPC calls return. Now I have revised my Raft and uses Test Script to test Lab 2 A + B + C 90 times, and result is passed all 90 tests.(I revised my code and continued my test from 10th test, 2 failed is past recorded) This time, I have some new things to write about In fact, we do not need to set extract timer for RequestVote RPC, I once set [50,100]ms range to wait for RPC return, but test result turned out not so well. What we actually need to do is reset timer before start a new election round (So if RPC not return on time, timer will just start next election) Apply interval time need to be considered carefully, because I once set 1000ms to apply new log to state machine, result are worse than 200ms’ When you are encountered with a livelock, check your RequestVote handler. I implemented this function mistakenly and led to the follower with up-to-data log cannot win the election on time. (Specifically, you need to make sure your peer convert to follower whenever it receives a RPC whose term is greater than currentTerm) Check if your output channels are blocked and without receivers. Just uses channel with buffer(Can anybody tell me if go can GC unused channel buffer after both sender and receiver quit?) Oh, I finally did not revise my election part using condition variable, just used channel. ​ Result of a new 100 tests","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Lab Note","slug":"Lab-Note","permalink":"https://zjuytw.github.io/tags/Lab-Note/"},{"name":"Raft","slug":"Raft","permalink":"https://zjuytw.github.io/tags/Raft/"}]},{"title":"6.824 Lab1 MapReduce","slug":"6.824-lab1-MapReduce","date":"2021-08-09T08:40:40.263Z","updated":"2021-08-09T10:06:23.719Z","comments":true,"path":"2021/08/09/6.824-lab1-MapReduce/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/6.824-lab1-MapReduce/","excerpt":"","text":"MapReduce Summary Topics about Distributed Systems Infrastructure Storage Communication Computation Performance Scalability Fault Tolerance Consistency strong consistency weak consistency MapReduce About MapReduce TL;DR – Google’s engineers build a model that implements the split and dispatch process to make large scale computation tasks easy programmed. (Just need to write map() and reduce() function) ​ You can find the detailed model description in MapReduce: Simplified Data Processing on Large Clusters Brief Execution flow Split the input files into M splits, which is generally 16-64 MB in size. And they are the basic unit one worker handles We set up a master process to schedule the workflows of each worker, it will pass the split to idle worker (In fact the true process should be worker calls the master for a job) Worker do their Map() job, and the output of Map() should be &lt;key, value&gt; pair. Store the Intermediate output on local disks (like json,etc). AND!! Here is a trick that usually we need have R final reduced files, so we have to partition these &lt;key,value&gt; pairs into R parts. (Use hash(key) mod R) Master received the intermediate files locations and once all the input files are processed. Master will assign Reduce() job to workers Workers do the Reduce() job and output the final result on disk ​ Some Hints on Lab1 Claim: Below is all my own solution, and maybe happen to pass all tests. :) Enviornment Requirement GOLANG, You shall have basic golang knowledge like slice, map ‘s insert, delete, find… Also should know a little bit concurrency programming(just know what mutex mean hah), RPC programming… linux 64bit For Chinese mainland students, if you are using vscode, you may have trouble install golang extensions in vscode. Google the problem, CSDN sucks About master.go Checked input files map when a worker calls for job, also applicable for reduce job Recovery Problem: in master, I run a timecount gorountine for each assigned job, if the worker does not report on time, just modify todo list and assign it to the later worker( of course the former worker’s job is abandoned) If all map tasks are assigned, but not completed, just tell idle workers WAIT. If reduced tasks are all done, tell idle workers QUIT Don’t forget to lock up the critical area About worker.go use a loop to constantly getJob, and if master tells worker QUIT, then quit(Also WAIT, then wait) Because each worker just read the seperate file, there is no synchronize problem when read But for these worker on write, there is chance that abandoned worker is write the same file alone with new assigned workerYou should be careful when write the intermediate/output file About lambda function with goroutine ​ Use my goroutine function for example* When write lamda function, there is a trick that you should write these variable which you don’t want them be changed outside the lambda funtion as function’s parameter This is because that compiler will put the local variable(these used in lambda function) on heap NOT STACK, so once the variable is changed outside, the thread will read the changed variable. This is sometimes you don’t wanna see.","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Lab Note","slug":"Lab-Note","permalink":"https://zjuytw.github.io/tags/Lab-Note/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://zjuytw.github.io/tags/MapReduce/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-08-09T03:10:31.995Z","updated":"2021-08-09T04:07:37.526Z","comments":true,"path":"2021/08/09/hello-world/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"5841. Find the Longest Valid Obstacle Course at Each Position","slug":"5841. Find the Longest Valid Obstacle Course at Each Position","date":"2021-08-08T17:04:57.677Z","updated":"2021-08-09T08:34:33.477Z","comments":true,"path":"2021/08/09/5841. Find the Longest Valid Obstacle Course at Each Position/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/5841.%20Find%20the%20Longest%20Valid%20Obstacle%20Course%20at%20Each%20Position/","excerpt":"","text":"5841. Find the Longest Valid Obstacle Course at Each Position tag: monotone-stack, greedy, Maximum Increasing Subsequence Description Solution Use monotone-stack to find the longest subsequence end with obstacles[i] Greedily replace the very obstacles[j], j &lt; i that exactly greater than obstacles[i], other elements in the stack just remain 12341 3 5 7 4stack : 1 3 5 7after : 1 3 4 7 Just be careful about the same number should also be inclueded, so just binary search for (obstacle[i] + 1) Code 123456789101112131415161718class Solution &#123;public: vector&lt;int&gt; longestObstacleCourseAtEachPosition(vector&lt;int&gt;&amp; obstacles) &#123; vector&lt;int&gt; dp; vector&lt;int&gt; res; for(int i = 0; i &lt; obstacles.size(); i++)&#123; auto pos = lower_bound(dp.begin(), dp.end(), obstacles[i]+1); if(pos != dp.end())&#123; *pos = obstacles[i]; res.push_back(pos - dp.begin() + 1); &#125;else&#123; dp.push_back(obstacles[i]); res.push_back(dp.size()); &#125; &#125; return res; &#125;&#125;;","categories":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://zjuytw.github.io/categories/Leetcode/"}],"tags":[{"name":"monotone-stack","slug":"monotone-stack","permalink":"https://zjuytw.github.io/tags/monotone-stack/"},{"name":"greedy","slug":"greedy","permalink":"https://zjuytw.github.io/tags/greedy/"},{"name":"maximum increasing subsequence","slug":"maximum-increasing-subsequence","permalink":"https://zjuytw.github.io/tags/maximum-increasing-subsequence/"}]},{"title":"5840. Minimum Number of Swaps to Make the String Balanced","slug":"5840. Minimum Number of Swaps to Make the String Balanced","date":"2021-08-08T17:00:27.810Z","updated":"2021-08-09T08:34:33.476Z","comments":true,"path":"2021/08/09/5840. Minimum Number of Swaps to Make the String Balanced/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/5840.%20Minimum%20Number%20of%20Swaps%20to%20Make%20the%20String%20Balanced/","excerpt":"","text":"5840. Minimum Number of Swaps to Make the String Balanced tag: stack, greedy Description Solution In each switch, brackets are reduced mostly 2, at least 1. 123//Just swap the first dismatched ] with second dismatched [2 for: ]]] [[[ -&gt; []] [][. 1 for just 1 pair left, switch them then all done Code 12345678910111213141516171819class Solution &#123;public: int minSwaps(string s) &#123; //只要[的右边有对应个数个]即可 stack&lt;int&gt; stk; int res = 0; for(int i = 0; i &lt; s.size(); i++)&#123; if(s[i] == &#x27;[&#x27;)&#123; stk.push(i); &#125;else&#123; if(stk.empty()) res++; else stk.pop(); &#125; &#125; return res - res/2; &#125;&#125;;","categories":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://zjuytw.github.io/categories/Leetcode/"}],"tags":[{"name":"greedy","slug":"greedy","permalink":"https://zjuytw.github.io/tags/greedy/"},{"name":"stack","slug":"stack","permalink":"https://zjuytw.github.io/tags/stack/"}]},{"title":"1792.Maximum Average Pass Ratio","slug":"1792. Maximum Average Pass Ratio","date":"2021-08-08T16:54:36.226Z","updated":"2021-08-09T08:34:33.475Z","comments":true,"path":"2021/08/09/1792. Maximum Average Pass Ratio/","link":"","permalink":"https://zjuytw.github.io/2021/08/09/1792.%20Maximum%20Average%20Pass%20Ratio/","excerpt":"","text":"1792.Maximum Average Pass Ratio tag : Heap, No AC first time Description Solution I feel shamed for I failed to AC it at first time… Use a heap to store the whole develop rate for each class, and find the max dr and use it. O(NlogN) Code 1234567891011121314151617181920212223class Solution &#123;public: double maxAverageRatio(vector&lt;vector&lt;int&gt;&gt;&amp; classes, int extraStudents) &#123; priority_queue&lt;pair&lt;double, int&gt;, vector&lt;pair&lt;double, int&gt;&gt;, less&lt;&gt;&gt; pq; double passrate = 0; for(int i = 0; i &lt; classes.size(); i++)&#123; passrate += 1.0 * classes[i][0] / classes[i][1]; double pr = calPR(classes[i][0]++,classes[i][1]++); pq.emplace(pr, i); &#125; while(extraStudents--)&#123; auto [dr, index] = pq.top(); pq.pop(); passrate += dr; pq.emplace(calPR(classes[index][0]++, classes[index][1]++), index); &#125; return passrate/classes.size(); &#125; double calPR(int p, int t)&#123; return 1.0 * (p + 1) / (t + 1) - 1.0 * p / t; &#125;&#125;;","categories":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://zjuytw.github.io/categories/Leetcode/"}],"tags":[{"name":"No AC first time","slug":"No-AC-first-time","permalink":"https://zjuytw.github.io/tags/No-AC-first-time/"},{"name":"Heap","slug":"Heap","permalink":"https://zjuytw.github.io/tags/Heap/"}]},{"title":"1036. Escape a Large Maze","slug":"1036. Escape a Large Maze","date":"2021-08-07T14:12:50.626Z","updated":"2021-08-09T08:34:33.474Z","comments":true,"path":"2021/08/07/1036. Escape a Large Maze/","link":"","permalink":"https://zjuytw.github.io/2021/08/07/1036.%20Escape%20a%20Large%20Maze/","excerpt":"","text":"1036. Escape a Large Maze tag: BFS Description Solution BFS + early quit. Because 1M * 1M is too large for BFS, so we need to find a way to return quickly. blocked.length &lt;= 200 is a good quality we can look into. In a square, the best way to lock an area is laying all blocks 45° as following: 1234567890th _________________________ |O O O O O O O X |O O O O O O X |O O O O O X |O O O O X .O O O X .O O X .O X 200th |X And there are maximally (199 + 1) * 199 /2 = 19900 grids. The exceeding of this number means we can not block one source. So we can return quickly by determine if 19901’s grid has been visited. Code 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; int dir[5] = &#123;1, 0, -1, 0 , 1&#125;; public: bool isEscapePossible(vector&lt;vector&lt;int&gt;&gt;&amp; blocked, vector&lt;int&gt;&amp; source, vector&lt;int&gt;&amp; target) &#123; set&lt;pair&lt;int,int&gt;&gt; blocks; for(auto block : blocked) blocks.emplace(block[0],block[1]); return bfs(source, blocks, target) &amp;&amp; bfs(target, blocks, source); &#125; bool bfs(vector&lt;int&gt;&amp; source, set&lt;pair&lt;int,int&gt;&gt; blocks, vector&lt;int&gt; &amp;target)&#123; queue&lt;pair&lt;int,int&gt;&gt; q1; q1.emplace(source[0], source[1]); set&lt;pair&lt;int,int&gt;&gt; seen; seen.insert(&#123;source[0], source[1]&#125;); while(!q1.empty())&#123; int size = q1.size(); for(int i = 0; i &lt; size; i ++)&#123; auto [r,c] = q1.front(); q1.pop(); for(int j = 1; j &lt; 5; j++)&#123; int x = r + dir[j-1], y = c + dir[j]; if(x &gt;= 0 &amp;&amp; x &lt; 1E6 &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; 1E6)&#123; if(!seen.count(&#123;x,y&#125;) &amp;&amp; !blocks.count(&#123;x,y&#125;))&#123; if(x == target[0] &amp;&amp; y == target[1]) return true; q1.emplace(x,y); seen.insert(&#123;x,y&#125;); &#125; &#125; &#125; &#125; if(seen.size() &gt;= 19901)&#123; cout &lt;&lt; &quot;Oversized&quot; &lt;&lt; endl; return true; &#125; &#125; return false; &#125;&#125;;","categories":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://zjuytw.github.io/categories/Leetcode/"}],"tags":[{"name":"BFS","slug":"BFS","permalink":"https://zjuytw.github.io/tags/BFS/"},{"name":"No AC first time","slug":"No-AC-first-time","permalink":"https://zjuytw.github.io/tags/No-AC-first-time/"}]},{"title":"1671. Minimum Number of Removals to Make Mountain Array","slug":"1671. Minimum Number of Removals to Make Mountain Array","date":"2021-08-07T13:13:10.709Z","updated":"2021-08-09T08:34:33.474Z","comments":true,"path":"2021/08/07/1671. Minimum Number of Removals to Make Mountain Array/","link":"","permalink":"https://zjuytw.github.io/2021/08/07/1671.%20Minimum%20Number%20of%20Removals%20to%20Make%20Mountain%20Array/","excerpt":"","text":"1671. Minimum Number of Removals to Make Mountain Array Tag: monotone-stack, DP, No AC first time Description Solution For each index, we can calculate its preceding minimum delete number and its succeeding delete number. Then search for the minimum sum. So how to calculate deleting number? -&gt; calculate longest monotonous sequence A naive way, use DP to calculate: Find the very nums[j] that lower than nums[i], then inherit its by length + 1 1dp[i] = max(dp[j] + 1) for all nums[j] &lt; nums[i] A more efficient way is to maintain a monotone-stack, and greedily replace the very nums[j] that exact greater than nums[i]. Example: 12341 3 5 7 4stack : 1 3 5 7after : 1 3 4 7 ​ So that we can maximally insert number into this monotone-stack which means can get the longest sequence. Code 1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123;public: int minimumMountainRemovals(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;pair&lt;int,int&gt;&gt; dp(n,&#123;0,0&#125;); vector&lt;int&gt; stk; for(int i = 0; i &lt; n; i++)&#123; auto pos = lower_bound(stk.begin(), stk.end(), nums[i]); if(pos != stk.end())&#123; *pos = nums[i]; dp[i].first = pos - stk.begin() + 1; &#125;else&#123; stk.push_back(nums[i]); dp[i].first = stk.size(); &#125; &#125; while(!stk.empty())&#123; stk.pop_back(); &#125; for(int i = n-1; i &gt;= 0; i--)&#123; auto pos = lower_bound(stk.begin(), stk.end(), nums[i]); if(pos != stk.end())&#123; *pos = nums[i]; dp[i].second = pos - stk.begin() + 1; &#125;else&#123; stk.push_back(nums[i]); dp[i].second = stk.size(); &#125; &#125; int res = INT_MAX; for(int i = 1; i &lt; n -1; i++)&#123; if(dp[i].first &gt;= 2 &amp;&amp; dp[i].second &gt;= 2)&#123; res = min(res, n - dp[i].first - dp[i].second + 1); &#125; &#125; return res; &#125;&#125;;","categories":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://zjuytw.github.io/categories/Leetcode/"}],"tags":[{"name":"No AC first time","slug":"No-AC-first-time","permalink":"https://zjuytw.github.io/tags/No-AC-first-time/"},{"name":"monotone-stack","slug":"monotone-stack","permalink":"https://zjuytw.github.io/tags/monotone-stack/"},{"name":"greedy","slug":"greedy","permalink":"https://zjuytw.github.io/tags/greedy/"},{"name":"DP","slug":"DP","permalink":"https://zjuytw.github.io/tags/DP/"}]},{"title":"457. Circular Array Loop","slug":"457. Circular Array Loop","date":"2021-08-07T04:17:12.129Z","updated":"2021-08-09T08:34:33.476Z","comments":true,"path":"2021/08/07/457. Circular Array Loop/","link":"","permalink":"https://zjuytw.github.io/2021/08/07/457.%20Circular%20Array%20Loop/","excerpt":"","text":"457. Circular Array Loop Tag: Fast-Slow pointers, No AC first time Description Solution To determine a cycle, Fast-Slow pointers is a good way for solving it in linear time and constant space. Some tricky points Determine all positive or all negative Determine length k&gt;1 We can do a product of two nums to judge whether they are same positive or not, and do slow == Next(slow) to judge loop’s length == 1 Code 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123;public: bool circularArrayLoop(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); for(int i = 0; i &lt; nums.size(); i++)&#123; if(nums[i] == 0) continue; int fast = getNext(n, i, nums[i]), slow = i; bool pos = nums[i] &gt; 0, res = true; while(nums[slow] * nums[fast] &gt; 0 &amp;&amp; nums[slow] * nums[getNext(n, fast, nums[fast])] &gt; 0)&#123; if(slow == fast)&#123; if(slow != getNext(n, slow, nums[slow])) return true; break; &#125; slow = getNext(n,slow, nums[slow]); fast = getNext(n, fast, nums[fast]); fast = getNext(n, fast, nums[fast]); &#125; int tmp = i; while(nums[tmp] * nums[getNext(n, tmp, nums[tmp])] &gt; 0)&#123; int step = nums[tmp]; nums[tmp] = 0; tmp = getNext(n, tmp, step); &#125; &#125; return false; &#125; int getNext(int size, int i, int move)&#123; while(i + move &lt; 0) i += size; while(i + move &gt;= size) i -= size; return i + move; &#125;&#125;;","categories":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://zjuytw.github.io/categories/Leetcode/"}],"tags":[{"name":"No AC first time","slug":"No-AC-first-time","permalink":"https://zjuytw.github.io/tags/No-AC-first-time/"},{"name":"Fast-Slow pointers","slug":"Fast-Slow-pointers","permalink":"https://zjuytw.github.io/tags/Fast-Slow-pointers/"}]},{"title":"847. Shortest Path Visiting All Nodes","slug":"847. Shortest Path Visiting All Nodes","date":"2021-08-07T02:42:37.295Z","updated":"2021-08-09T08:34:33.477Z","comments":true,"path":"2021/08/07/847. Shortest Path Visiting All Nodes/","link":"","permalink":"https://zjuytw.github.io/2021/08/07/847.%20Shortest%20Path%20Visiting%20All%20Nodes/","excerpt":"","text":"847. Shortest Path Visiting All Nodes Tag: State Compression, BFS, No AC first time Description Solutions We can see from constrains that n&lt;=12, so we can use state compression. Also, the weight of each edge is 1, which reminds us of BFS to search for the lowest distance to reach final state 1&lt;&lt;n - 1 Some tricky points Use tuple to store a three tuple, &#123;node, mask, dist&#125; for the current node, mask and current distance. Use a array or map to store the visited state, states should be distinct by their mask and current node. Code 12345678910111213141516171819202122232425262728293031class Solution &#123;public: int shortestPathLength(vector&lt;vector&lt;int&gt;&gt;&amp; graph) &#123; int n = graph.size(); queue&lt;tuple&lt;int,int,int&gt;&gt; q; //&#123;node, mask, dist&#125; vector&lt;vector&lt;int&gt;&gt; seen(n, vector&lt;int&gt;(1 &lt;&lt; n, 0)); for(int i = 0; i &lt; n; i++)&#123; q.emplace(i, 1 &lt;&lt; i, 0); seen[i][1&lt;&lt;i] = 1; &#125; int ans = 0; while(!q.empty())&#123; auto [u, mask, dist] = q.front(); q.pop(); if(mask == (1 &lt;&lt; n) - 1)&#123; ans = dist; break; &#125; //search adjecent nodes for(auto next : graph[u])&#123; int nxtMask = mask | 1 &lt;&lt; next; if(!seen[next][nxtMask])&#123; q.emplace(next, nxtMask, dist+1); seen[next][nxtMask] = true; &#125; &#125; &#125; return ans; &#125;&#125;;","categories":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://zjuytw.github.io/categories/Leetcode/"}],"tags":[{"name":"BFS","slug":"BFS","permalink":"https://zjuytw.github.io/tags/BFS/"},{"name":"No AC first time","slug":"No-AC-first-time","permalink":"https://zjuytw.github.io/tags/No-AC-first-time/"},{"name":"State Compression","slug":"State-Compression","permalink":"https://zjuytw.github.io/tags/State-Compression/"}]},{"title":"6.824 Lab2 Raft","slug":"6.824-lab2-Raft","date":"2021-06-15T05:00:12.685Z","updated":"2021-08-09T10:06:33.377Z","comments":true,"path":"2021/06/15/6.824-lab2-Raft/","link":"","permalink":"https://zjuytw.github.io/2021/06/15/6.824-lab2-Raft/","excerpt":"","text":"Raft Paper Topics Start with a Raft Protocol paper, In Search of an Understandable Consensus Alogorithm , Also you can find Chinese version here Raft Basics Contain 3 kinds of servers : Follower, Candidate, Leader The Leader will be elected for each &lt;**term**&gt;, and term is the basic unit served as a Timer to denote the up-to-date log’s time stamp Leader Election Using the election timeout mechanism to make Follower begin a new round elction To avoid split votes, Raft set the random election timeout for those server who calls a RequestVote() but one RPC call fails maybe due to the destination disconnect. Log replication Once become leader, leader receives command from client and appends the command to its log. Then calls AppendEntries RPC in parallel to notify followers Lab2A In this part, we gonna establish a simplified raft just implements the leader election and leader sending heartbeat to maintain its status. My implementation Set variables for each structure described in Figure 2. of the former paper Raft Struct &amp; RequestVote RPC &amp;AppendEntries RPC, For Example: 12345678910111213141516171819202122type Raft struct &#123; mu sync.Mutex // Lock to protect shared access to this peer&#x27;s state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer&#x27;s persisted state me int // this peer&#x27;s index into peers[] dead int32 // set by Kill() // Your data here (2A, 2B, 2C). checkHeartBeat bool currentTerm int votedFor int log []int //Volatile commitIndex int lastApplied int //For leaders nextIndex []int matchIndex []int status int&#125; Implement Make() 123456789101112131415161718192021func Make(...) *Raft&#123; initRf() go checkHeartBeat()&#123; // if received Leader&#x27;s HeartBeat then Sleep, nor begin a new election time.Sleep() if !heartBeatChecked() &amp;&amp; !isLeader()&#123; //timeOutElection part for each server in peers do go sendRequestVote(server)// need set a timeout thread to makesure no longterm waiting done ... collectVoteInfo()&#123; if receives majority vote, become leader doLeader() else back to Follower &#125; &#125; &#125;&#125; Hints MUST follow which variables paper described in data structure set mutex whenever there is a multithread R/W condition when call RPC, you should be aware of that caller will congest until RPC returns a true or false( which will take a long time), a good way to solve this is create a goroutine (For HeartBeat or RequestVote). To find this congestion took me long time repeating running test, I hope you won’t. A simple script test of run 20 times ‘go -test run 2A’ Lab2B This Part is much more difficult than Lab2A… In 2B, we are about to implement AppendEntries. Specifically, for leader, it should send new log periodically to each follower(leader maintains each follower’s nextIndex to send), for followers, check consistency of RPC’s logs with attached prevLog, if prevLog doesn’t match local log, reject this append and response to leader Fault Tolerance is the core part we should pay much attention to Commit the log to state machine whenever commitIndex is increased We can not emphasize more the importance of stick to paper’s figure2 !!! Hints I revised my checkHeartBeat() as a timer, and the callback function is the timeOutElection I can not sure the exact performance difference between former design and current one, but there are two point I want to claim Using the timer, it can make sure your callback function runs more periodically. Consider if using goroutine, you will always start a timeout count after last timeout election ends. This time lag may result a livelock Make sure timer won’t trigger callback when a peer receives RequestVote or once a leader is elected, some other node starts an election just because a little time latency to reset timer, forcing the recently elected leader to abdicate immediately Set the election time out a right random range Test Result ​ single test ​ batch test Lab2C If your former code is implemented well and free of bug, it is easy to complete persist() and readPersist() by reading example in comments. Hints Check for 2B and 2A content if your code fails to pass the test. You should do persist() as long as your non-volatile variable is changed","categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"}],"tags":[{"name":"Lab Note","slug":"Lab-Note","permalink":"https://zjuytw.github.io/tags/Lab-Note/"},{"name":"Raft","slug":"Raft","permalink":"https://zjuytw.github.io/tags/Raft/"}]}],"categories":[{"name":"6.824","slug":"6-824","permalink":"https://zjuytw.github.io/categories/6-824/"},{"name":"Leetcode","slug":"Leetcode","permalink":"https://zjuytw.github.io/categories/Leetcode/"}],"tags":[{"name":"Distributed Systems","slug":"Distributed-Systems","permalink":"https://zjuytw.github.io/tags/Distributed-Systems/"},{"name":"Bitcoin","slug":"Bitcoin","permalink":"https://zjuytw.github.io/tags/Bitcoin/"},{"name":"Certificate Transparency","slug":"Certificate-Transparency","permalink":"https://zjuytw.github.io/tags/Certificate-Transparency/"},{"name":"COPS","slug":"COPS","permalink":"https://zjuytw.github.io/tags/COPS/"},{"name":"Memcache@FB","slug":"Memcache-FB","permalink":"https://zjuytw.github.io/tags/Memcache-FB/"},{"name":"Spark","slug":"Spark","permalink":"https://zjuytw.github.io/tags/Spark/"},{"name":"FaRM","slug":"FaRM","permalink":"https://zjuytw.github.io/tags/FaRM/"},{"name":"Spanner","slug":"Spanner","permalink":"https://zjuytw.github.io/tags/Spanner/"},{"name":"Frangipani","slug":"Frangipani","permalink":"https://zjuytw.github.io/tags/Frangipani/"},{"name":"Aurora","slug":"Aurora","permalink":"https://zjuytw.github.io/tags/Aurora/"},{"name":"CRAQ","slug":"CRAQ","permalink":"https://zjuytw.github.io/tags/CRAQ/"},{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://zjuytw.github.io/tags/ZooKeeper/"},{"name":"Lab Note","slug":"Lab-Note","permalink":"https://zjuytw.github.io/tags/Lab-Note/"},{"name":"ShardKV Storage","slug":"ShardKV-Storage","permalink":"https://zjuytw.github.io/tags/ShardKV-Storage/"},{"name":"KV Raft","slug":"KV-Raft","permalink":"https://zjuytw.github.io/tags/KV-Raft/"},{"name":"Raft","slug":"Raft","permalink":"https://zjuytw.github.io/tags/Raft/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://zjuytw.github.io/tags/MapReduce/"},{"name":"monotone-stack","slug":"monotone-stack","permalink":"https://zjuytw.github.io/tags/monotone-stack/"},{"name":"greedy","slug":"greedy","permalink":"https://zjuytw.github.io/tags/greedy/"},{"name":"maximum increasing subsequence","slug":"maximum-increasing-subsequence","permalink":"https://zjuytw.github.io/tags/maximum-increasing-subsequence/"},{"name":"stack","slug":"stack","permalink":"https://zjuytw.github.io/tags/stack/"},{"name":"No AC first time","slug":"No-AC-first-time","permalink":"https://zjuytw.github.io/tags/No-AC-first-time/"},{"name":"Heap","slug":"Heap","permalink":"https://zjuytw.github.io/tags/Heap/"},{"name":"BFS","slug":"BFS","permalink":"https://zjuytw.github.io/tags/BFS/"},{"name":"DP","slug":"DP","permalink":"https://zjuytw.github.io/tags/DP/"},{"name":"Fast-Slow pointers","slug":"Fast-Slow-pointers","permalink":"https://zjuytw.github.io/tags/Fast-Slow-pointers/"},{"name":"State Compression","slug":"State-Compression","permalink":"https://zjuytw.github.io/tags/State-Compression/"}]}